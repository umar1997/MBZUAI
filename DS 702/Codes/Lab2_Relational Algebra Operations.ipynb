{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d62b51",
   "metadata": {
    "id": "37d62b51"
   },
   "source": [
    "# Relational Algebra Operations. MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57222440",
   "metadata": {
    "id": "57222440"
   },
   "source": [
    "There are a number of operations on large-scale data that are used in database queries. Many traditional database applications involve retrieval of small amounts of data, even though the database itself may be large. For example, a query may ask for the bank balance of one particular account. Such queries are not useful applications of MapReduce. \n",
    "\n",
    "However, there are many operations on data that can be described easily in terms of the common database-query primitives, even if the queries themselves are not executed within a database management system. Thus, a good starting point for exploring applications of MapReduce is by considering the standard operations on relations. We assume you are familiar with database systems, the query language SQL, and the relational model, but to review, a ***relation*** is a table with column headers called ***attributes***. Rows of the relation are called ***tuples***. The set of attributes of a relation is called its ***schema***. \n",
    "\n",
    "There are several standard operations on relations, often referred to as ***relational algebra***, that are used to implement queries. The queries themselves usually are written in SQL. The relational-algebra operations we will discuss are:\n",
    "\n",
    "- selection\n",
    "- projection\n",
    "- union, intersection and difference\n",
    "- natural join\n",
    "- grouping and aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588ff42",
   "metadata": {
    "id": "e588ff42"
   },
   "source": [
    "## Selection operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7359d0",
   "metadata": {
    "id": "5e7359d0"
   },
   "source": [
    "Apply a condition C to each tuple in the relation and produce as output only those tuples that satisfy C. \n",
    "\n",
    "**Operation:** $Select_C(R)$.\n",
    "\n",
    "**Map:** For each tuple $t$ in $R$, test if it satisfies $C$. If so, produce the key-value pair $(t, t)$. That is, both the key and value are $t$. \n",
    "\n",
    "**Reduce:** The Reduce function is the identity. It simply passes each key-value pair to the output.\n",
    "\n",
    "|        | Input          | Output       |\n",
    "|--------|----------------|--------------|\n",
    "| map    | <k1, t>        | list(<t, t>) |\n",
    "| reduce | (<t, list(t)>) | list(<t, t>) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d24c57",
   "metadata": {
    "id": "62d24c57"
   },
   "source": [
    "Let's create a custom data to work on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205992ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umar/.local/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba9fd73",
   "metadata": {
    "id": "1ba9fd73"
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 2), (2, 3), (5, 6), (2, 8), (4, 4), (6, 1), # (1, 2), \n",
    "    (6, 2), (6, 3), (7, 8), (9, 8), (3, 3), (0, 1)\n",
    "]\n",
    "cols = ['A', 'B']\n",
    "df = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d13a7",
   "metadata": {
    "id": "788d13a7"
   },
   "source": [
    "`Selection(B <= 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1196068e",
   "metadata": {
    "id": "1196068e"
   },
   "outputs": [],
   "source": [
    "df_map = df.filter(lambda x: x[1] <= 3).map(lambda x: (x, x)) # Notice (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d0b3f2",
   "metadata": {
    "id": "42d0b3f2",
    "outputId": "01d4ee25-5539-482f-fd88-1afd33b9e5a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), (1, 2)),\n",
       " ((2, 3), (2, 3)),\n",
       " ((6, 1), (6, 1)),\n",
       " ((6, 2), (6, 2)),\n",
       " ((6, 3), (6, 3)),\n",
       " ((3, 3), (3, 3)),\n",
       " ((0, 1), (0, 1))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d462f3",
   "metadata": {},
   "source": [
    "#### Reduce By Key\n",
    "Spark RDD reduceByKey() transformation is used to merge the values of each key using an associative reduce function.  It is a wider transformation as it shuffles data across multiple partitions and it operates on pair RDD (key/value pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7dd475",
   "metadata": {
    "id": "2e7dd475"
   },
   "outputs": [],
   "source": [
    "# df_res = df_map.map(lambda x: x[1]) # reduce's place \n",
    "df_res = df_map.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364dd0c8",
   "metadata": {
    "id": "364dd0c8",
    "outputId": "569bbf3b-870c-45d8-badd-f4134e7b5d77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3), (1, 2), (2, 3), (6, 3), (0, 1), (6, 2), (6, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f367f0",
   "metadata": {
    "id": "70f367f0"
   },
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95459e12",
   "metadata": {
    "id": "95459e12"
   },
   "source": [
    "For some subset S of the attributes of the relation, produce from each tuple only the components for the attributes in S.\n",
    "\n",
    "**Operation:** $Project_A(R)$.\n",
    "\n",
    "**Map:** For each tuple $t$ in $R$, construct a tuple $t'$ by eliminating from t those components whose attributes are not in A. Output the key-value pair $(t', t')$.\n",
    "\n",
    "\n",
    "**Reduce:** For each key $t'$ produced by any of the Map tasks, there will be one or more key-value pairs ($t'$, $t'$). The Reduce function turns $(t', [t', t', . . . , t'])$ into $(t', t')$, so it produces exactly one pair $(t', t')$ for this key $t'$.\n",
    "\n",
    "|        | Input                           | Output             |\n",
    "|--------|---------------------------------|--------------------|\n",
    "| map    | <k1, t>                         | list(<$t'$, $t'$>) |\n",
    "| reduce | (<$t'$, list($t'$, ..., $t'$)>) | list(<$t'$, $t'$>) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8e1198",
   "metadata": {
    "id": "ca8e1198"
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 2, 3), (2, 2, 2), (1, 2, 1), (4, 2, 1), (6, 8, 4), (3, 2, 2), \n",
    "    (1, 2, 5), (2, 3, 2), (1, 3, 1), (3, 2, 1), (6, 8, 9), (3, 4, 2)\n",
    "]\n",
    "cols = ['A', 'B', 'C']\n",
    "df = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d6f7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3),\n",
       " (2, 2, 2),\n",
       " (1, 2, 1),\n",
       " (4, 2, 1),\n",
       " (6, 8, 4),\n",
       " (3, 2, 2),\n",
       " (1, 2, 5),\n",
       " (2, 3, 2),\n",
       " (1, 3, 1),\n",
       " (3, 2, 1),\n",
       " (6, 8, 9),\n",
       " (3, 4, 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddae2c",
   "metadata": {
    "id": "38ddae2c"
   },
   "source": [
    "Here, we are going to choose only first second columns and drop the third column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7059c91b",
   "metadata": {
    "id": "7059c91b",
    "outputId": "1ea8b54c-2ea1-49ad-bb9e-f25eefac5467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1, 2), (1, 2)), ((2, 2), (2, 2)), ((1, 2), (1, 2)), ((4, 2), (4, 2)), ((6, 8), (6, 8)), ((3, 2), (3, 2)), ((1, 2), (1, 2)), ((2, 3), (2, 3)), ((1, 3), (1, 3)), ((3, 2), (3, 2)), ((6, 8), (6, 8)), ((3, 4), (3, 4))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((4, 2), <pyspark.resultiterable.ResultIterable at 0x7f8b38f40ba8>),\n",
       " ((6, 8), <pyspark.resultiterable.ResultIterable at 0x7f8b38f40780>),\n",
       " ((1, 2), <pyspark.resultiterable.ResultIterable at 0x7f8b38f40908>),\n",
       " ((2, 3), <pyspark.resultiterable.ResultIterable at 0x7f8b38f406d8>),\n",
       " ((3, 4), <pyspark.resultiterable.ResultIterable at 0x7f8b38f40390>),\n",
       " ((2, 2), <pyspark.resultiterable.ResultIterable at 0x7f8b38f40668>),\n",
       " ((1, 3), <pyspark.resultiterable.ResultIterable at 0x7f8b38f40160>),\n",
       " ((3, 2), <pyspark.resultiterable.ResultIterable at 0x7f8b38f40128>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map = df.map(lambda x: ((x[0], x[1]), (x[0], x[1])))\n",
    "print(df_map.collect())\n",
    "\n",
    "\n",
    "y = df_map.groupByKey()\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b17b390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 2), [(4, 2)]),\n",
       " ((6, 8), [(6, 8), (6, 8)]),\n",
       " ((1, 2), [(1, 2), (1, 2), (1, 2)]),\n",
       " ((2, 3), [(2, 3)]),\n",
       " ((3, 4), [(3, 4)]),\n",
       " ((2, 2), [(2, 2)]),\n",
       " ((1, 3), [(1, 3)]),\n",
       " ((3, 2), [(3, 2), (3, 2)])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(key[0],[val for val in key[1]]) for key in df_map.groupByKey().collect()]\n",
    "\n",
    "# Same as .map(lambda x : (x[0], list(x[1]))) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b8cee9",
   "metadata": {
    "id": "86b8cee9",
    "outputId": "4ab5675c-09bd-4bf3-d6ca-bde65a3e6110"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 2), [(4, 2)]),\n",
       " ((6, 8), [(6, 8), (6, 8)]),\n",
       " ((1, 2), [(1, 2), (1, 2), (1, 2)]),\n",
       " ((2, 3), [(2, 3)]),\n",
       " ((3, 4), [(3, 4)]),\n",
       " ((2, 2), [(2, 2)]),\n",
       " ((1, 3), [(1, 3)]),\n",
       " ((3, 2), [(3, 2), (3, 2)])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map = df.map(lambda x: ((x[0], x[1]), (x[0], x[1]))).groupByKey().map(lambda x : (x[0], list(x[1]))) \n",
    "# mapValues(lambda x: list(x))\n",
    "df_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48362a0b",
   "metadata": {},
   "source": [
    "#### Map Values\n",
    "When we use map() with a Pair RDD, we get access to both Key & value. There are times we might only be interested in accessing the value(& not key). In those case, we can use mapValues() instead of map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6306ceac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 2), [(4, 2)]),\n",
       " ((6, 8), [(6, 8), (6, 8)]),\n",
       " ((1, 2), [(1, 2), (1, 2), (1, 2)]),\n",
       " ((2, 3), [(2, 3)])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataExtra = [\n",
    "    ((4, 2), [(4, 2)]),\n",
    "     ((6, 8), [(6, 8), (6, 8)]),\n",
    "     ((1, 2), [(1, 2), (1, 2), (1, 2)]),\n",
    "     ((2, 3), [(2, 3)])]\n",
    "dfExtra = sc.parallelize(dataExtra)\n",
    "dfExtra.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b476941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 2), (4, 2)), ((6, 8), (6, 8)), ((1, 2), (1, 2)), ((2, 3), (2, 3))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfExtra.mapValues(lambda x: x[0]).collect() \n",
    "# mapValues just alters the values and if you want to see only the values you use .values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a37a798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 2), (6, 8), (1, 2), (2, 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfExtra = dfExtra.mapValues(lambda x: x[0]).values()\n",
    "dfExtra.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a75f9876",
   "metadata": {
    "id": "a75f9876",
    "outputId": "262882e1-23e2-45de-db9a-be0a4875f9c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((4, 2), (4, 2)), ((6, 8), (6, 8)), ((1, 2), (1, 2)), ((2, 3), (2, 3)), ((3, 4), (3, 4)), ((2, 2), (2, 2)), ((1, 3), (1, 3)), ((3, 2), (3, 2))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4, 2), (6, 8), (1, 2), (2, 3), (3, 4), (2, 2), (1, 3), (3, 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_res = df_map.values().map(lambda x: x[0])\n",
    "\n",
    "# df_map printed above\n",
    "print(df_map.mapValues(lambda x: x[0]).collect())\n",
    "\n",
    "df_res = df_map.mapValues(lambda x: x[0]).values() \n",
    "df_res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836b60d",
   "metadata": {
    "id": "3836b60d"
   },
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b562a98",
   "metadata": {
    "id": "3b562a98"
   },
   "source": [
    "**Operation:** $Union(R, S)$.\n",
    "\n",
    "**Map:** Turn each input tuple $t$ either from relation $R$ or $S$ into a key-value pair $(t, t)$.\n",
    "\n",
    "\n",
    "**Reduce:** Associated with each key $t$ there will be either one or two values. Produce output $(t, t)$ in either case.\n",
    "\n",
    "|        | Input          | Output       |\n",
    "|--------|----------------|--------------|\n",
    "| map    | <k1, t>        | list(<t, t>) |\n",
    "| reduce | (<t, list(t)>) | list(<t, t>) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a413546",
   "metadata": {
    "id": "6a413546"
   },
   "outputs": [],
   "source": [
    "data1 = [(1, 2), (2, 3), (5, 6), (2, 3), (4, 4), (6, 1)]\n",
    "data2 = [(6, 1), (6, 3), (7, 8), (9, 8), (3, 3), (0, 1)]\n",
    "cols = ['A', 'B']\n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f366337",
   "metadata": {
    "id": "7f366337",
    "outputId": "31d21511-7936-412a-a50d-b22fd09a1cdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 2), (2, 3), (5, 6)], [(6, 1), (6, 3), (7, 8)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(3), rdd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dce9f48",
   "metadata": {
    "id": "6dce9f48",
    "outputId": "0d6e14f5-871e-4cfb-b39c-7ad39a039b7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), [(1, 2)]),\n",
       " ((2, 3), [(2, 3), (2, 3)]),\n",
       " ((5, 6), [(5, 6)]),\n",
       " ((4, 4), [(4, 4)]),\n",
       " ((6, 1), [(6, 1)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the map function and grouping in map reduce\n",
    "rdd1 = rdd1.map(lambda x: (x, x)).groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1752082",
   "metadata": {
    "id": "d1752082",
    "outputId": "48285c83-8b82-4daf-c4d7-4ce834467a43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 3), [(3, 3)]),\n",
       " ((6, 3), [(6, 3)]),\n",
       " ((7, 8), [(7, 8)]),\n",
       " ((0, 1), [(0, 1)]),\n",
       " ((6, 1), [(6, 1)]),\n",
       " ((9, 8), [(9, 8)])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd2.map(lambda x: (x, x)).groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2a75e",
   "metadata": {},
   "source": [
    "#### Reduce By Key\n",
    "Spark RDD reduceByKey() transformation is used to merge the values of each key using an associative reduce function.  It is a wider transformation as it shuffles data across multiple partitions and it operates on pair RDD (key/value pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de8c4162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), [(1, 2)]),\n",
       " ((2, 3), [(2, 3), (2, 3)]),\n",
       " ((5, 6), [(5, 6)]),\n",
       " ((4, 4), [(4, 4)]),\n",
       " ((6, 1), [(6, 1)]),\n",
       " ((3, 3), [(3, 3)]),\n",
       " ((6, 3), [(6, 3)]),\n",
       " ((7, 8), [(7, 8)]),\n",
       " ((0, 1), [(0, 1)]),\n",
       " ((6, 1), [(6, 1)]),\n",
       " ((9, 8), [(9, 8)])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.union(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e8d3248",
   "metadata": {
    "id": "0e8d3248",
    "outputId": "3eafa1ac-10ef-4caa-8ede-df9afa483eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[53] at RDD at PythonRDD.scala:53\n",
      "((1, 2), [(1, 2)])\n",
      "((5, 6), [(5, 6)])\n",
      "((6, 3), [(6, 3)])\n",
      "((0, 1), [(0, 1)])\n",
      "((4, 4), [(4, 4)])\n",
      "((6, 1), [(6, 1), (6, 1)])\n",
      "((9, 8), [(9, 8)])\n",
      "((3, 3), [(3, 3)])\n",
      "((2, 3), [(2, 3), (2, 3)])\n",
      "((7, 8), [(7, 8)])\n"
     ]
    }
   ],
   "source": [
    "print(rdd1.union(rdd2).reduceByKey(lambda x, y: x + y)) # Here X and Y are the value (list) with the same key\n",
    "\n",
    "for element in rdd1.union(rdd2).reduceByKey(lambda x, y: x + y).collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba288aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2),\n",
       " (5, 6),\n",
       " (6, 3),\n",
       " (0, 1),\n",
       " (4, 4),\n",
       " (6, 1),\n",
       " (9, 8),\n",
       " (3, 3),\n",
       " (2, 3),\n",
       " (7, 8)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.union(rdd2).reduceByKey(lambda x, y: x + y).mapValues(lambda x: x[0]).values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ac09e",
   "metadata": {
    "id": "4d8ac09e"
   },
   "source": [
    "## Intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983622be",
   "metadata": {
    "id": "983622be"
   },
   "source": [
    "**Operation:** $Intersection(R, S)$.\n",
    "\n",
    "**Map:** Turn each input tuple $t$ either from relation $R$ or $S$ into a key-value pair $(t, t)$.\n",
    "\n",
    "\n",
    "**Reduce:** If key $t$ has value  $list(t, t)$, then produce $(t, t)$. Otherwise, produce nothing.\n",
    "\n",
    "|        | Input                  | Output                          |\n",
    "|--------|------------------------|---------------------------------|\n",
    "| map    | <k1, t>                | list(<t, t>)                    |\n",
    "| reduce | $(<$$t$, $list(t)$$>)$ | list(<t, t>) if (<t, list(t, t)>) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5997efc9",
   "metadata": {
    "id": "5997efc9"
   },
   "outputs": [],
   "source": [
    "data1 = [(1, 2), (2, 3), (5, 6), (2, 3), (4, 4), (6, 1)]\n",
    "data2 = [(6, 1), (6, 3), (7, 8), (9, 8), (3, 3), (0, 1)]\n",
    "cols = ['A', 'B']\n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17b099e5",
   "metadata": {
    "id": "17b099e5",
    "outputId": "a04b9eb8-4c53-4d5b-a9c4-9525252a4be5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 2), (2, 3), (5, 6)], [(6, 1), (6, 3), (7, 8)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(3), rdd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d9199ff",
   "metadata": {
    "id": "2d9199ff",
    "outputId": "8c13cb9f-ef1c-47ad-a3b4-09b1013651ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), [(1, 2)]),\n",
       " ((2, 3), [(2, 3), (2, 3)]),\n",
       " ((5, 6), [(5, 6)]),\n",
       " ((4, 4), [(4, 4)]),\n",
       " ((6, 1), [(6, 1)])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd1.map(lambda x: (x, x)).groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d478b827",
   "metadata": {
    "id": "d478b827",
    "outputId": "0be36857-a5fb-4a9b-fcad-d9f6156a5ba5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 3), [(3, 3)]),\n",
       " ((6, 3), [(6, 3)]),\n",
       " ((7, 8), [(7, 8)]),\n",
       " ((0, 1), [(0, 1)]),\n",
       " ((6, 1), [(6, 1)]),\n",
       " ((9, 8), [(9, 8)])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd2.map(lambda x: (x, x)).groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac3cd392",
   "metadata": {
    "id": "ac3cd392",
    "outputId": "c3e77fed-0373-4b15-e787-a2cf730f409c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 1), (2, 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is wrong but Idky intersection doesnt work\n",
    "# Wrong because (2,3) isnt an intersection it just appears twice\n",
    "rdd1.union(rdd2).reduceByKey(lambda x, y: x + y).values().filter(lambda x: len(x) > 1).map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8038bb7",
   "metadata": {
    "id": "d8038bb7"
   },
   "source": [
    "## Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6abfe2",
   "metadata": {
    "id": "7c6abfe2"
   },
   "source": [
    "**Operation:** $Minus(R, S)$.\n",
    "\n",
    "**Map:** For each row `r` create a key-value pair `(r, T1)` if row is from table 1 else product key-value pair `(r, T2)`.\n",
    "\n",
    "\n",
    "**Reduce:** Output the row if and only if the value in the list is `T1` , otherwise output nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b90de62",
   "metadata": {
    "id": "1b90de62"
   },
   "outputs": [],
   "source": [
    "data1 = [(1, 2), (2, 3), (5, 6), (6, 1), (6, 3), (7, 8)]\n",
    "data2 = [(2, 3), (4, 4), (6, 1), (9, 8), (3, 3), (0, 1)]\n",
    "cols = ['A', 'B']\n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e8a3bdb",
   "metadata": {
    "id": "9e8a3bdb",
    "outputId": "f929f464-c898-4905-ae2c-acc40b2bfe08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (5, 6), (6, 1), (6, 3), (7, 8)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18407cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), <pyspark.resultiterable.ResultIterable at 0x7f8b38f48d30>),\n",
       " ((2, 3), <pyspark.resultiterable.ResultIterable at 0x7f8b38f439e8>),\n",
       " ((5, 6), <pyspark.resultiterable.ResultIterable at 0x7f8b38fb0588>),\n",
       " ((6, 3), <pyspark.resultiterable.ResultIterable at 0x7f8b38fb0d30>),\n",
       " ((7, 8), <pyspark.resultiterable.ResultIterable at 0x7f8b38fb0b38>),\n",
       " ((6, 1), <pyspark.resultiterable.ResultIterable at 0x7f8b38fb0a58>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda x: (x, 'T1')).groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee16e2c8",
   "metadata": {
    "id": "ee16e2c8",
    "outputId": "53c4ae59-efe2-4081-b47a-cc9b5b900136"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), ['T1']),\n",
       " ((2, 3), ['T1']),\n",
       " ((5, 6), ['T1']),\n",
       " ((6, 3), ['T1']),\n",
       " ((7, 8), ['T1']),\n",
       " ((6, 1), ['T1'])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .groupByKey() This gives a group by object\n",
    "# .mapValues(lambda x: list(x)) This gives the printability\n",
    "rdd1_map = rdd1.map(lambda x: (x, 'T1')).groupByKey().mapValues(lambda x: list(x))\n",
    "rdd1_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cfcd865",
   "metadata": {
    "id": "1cfcd865",
    "outputId": "3e9a03b4-1f79-4e16-f791-e5ee285177db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 3), ['T2']),\n",
       " ((2, 3), ['T2']),\n",
       " ((0, 1), ['T2']),\n",
       " ((4, 4), ['T2']),\n",
       " ((6, 1), ['T2']),\n",
       " ((9, 8), ['T2'])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2_map = rdd2.map(lambda x: (x, 'T2')).groupByKey().mapValues(lambda x: list(x))\n",
    "rdd2_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2764cd77",
   "metadata": {
    "id": "2764cd77",
    "outputId": "d11428b3-9cdf-4961-89dd-f3f656db478c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 3), ['T2']),\n",
       " ((1, 2), ['T1']),\n",
       " ((2, 3), ['T1', 'T2']),\n",
       " ((5, 6), ['T1']),\n",
       " ((6, 3), ['T1']),\n",
       " ((7, 8), ['T1']),\n",
       " ((0, 1), ['T2']),\n",
       " ((4, 4), ['T2']),\n",
       " ((6, 1), ['T1', 'T2']),\n",
       " ((9, 8), ['T2'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_res = (rdd1_map + rdd2_map).reduceByKey(lambda x, y: x + y) # In reduce by key you will see the x and y \n",
    "# are the values of the keys which are the same and the operation isbeing done on them\n",
    "rdd_res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5211078",
   "metadata": {
    "id": "d5211078",
    "outputId": "3e6b1913-3a17-4704-8bc4-b0477a130eed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (5, 6), (6, 3), (7, 8)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_res.filter(lambda x: x[1] == ['T1']).keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3502614",
   "metadata": {
    "id": "d3502614"
   },
   "source": [
    "## Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3b03d",
   "metadata": {
    "id": "eba3b03d"
   },
   "source": [
    "**Map:** For each row in the table, take the attributes using which grouping is to be done as the key, and value will be the ones on which aggregation is to be performed. For example, If a relation has 4 columns `A, B, C, D` and we want to group by `A`, `B` and do an aggregation on `C` we will make `(A, B)` as the key and `C` as the value.\n",
    "\n",
    "**Reduce:** Apply the aggregation operation (`sum, max, min, avg, …`) on the list of values and output the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03f5fe98",
   "metadata": {
    "id": "03f5fe98"
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 2, 3, 1), (2, 2, 2, 1), (1, 2, 1, 1), (4, 2, 1, 1), (6, 8, 4, 1), (3, 2, 2, 1), \n",
    "    (1, 2, 5, 1), (2, 3, 2, 1), (1, 3, 1, 1), (3, 2, 1, 1), (6, 8, 9, 1), (3, 4, 2, 1)\n",
    "]\n",
    "cols = ['A', 'B', 'C', 'D'] # just to show\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bcf72fc",
   "metadata": {
    "id": "8bcf72fc",
    "outputId": "41d80012-d906-455d-90cd-a4a4c1863ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3, 1), (2, 2, 2, 1), (1, 2, 1, 1), (4, 2, 1, 1), (6, 8, 4, 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b170769",
   "metadata": {
    "id": "1b170769",
    "outputId": "4a76166f-2e6a-4db7-f6d3-b768f113a28c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), 3),\n",
       " ((2, 2), 2),\n",
       " ((1, 2), 1),\n",
       " ((4, 2), 1),\n",
       " ((6, 8), 4),\n",
       " ((3, 2), 2),\n",
       " ((1, 2), 5),\n",
       " ((2, 3), 2),\n",
       " ((1, 3), 1),\n",
       " ((3, 2), 1),\n",
       " ((6, 8), 9),\n",
       " ((3, 4), 2)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: ((x[0], x[1]), x[2])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0735b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 2), [1]),\n",
       " ((6, 8), [4, 9]),\n",
       " ((1, 2), [3, 1, 5]),\n",
       " ((2, 3), [2]),\n",
       " ((3, 4), [2]),\n",
       " ((2, 2), [2]),\n",
       " ((1, 3), [1]),\n",
       " ((3, 2), [2, 1])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map = rdd.map(lambda x: ((x[0], x[1]), x[2])).groupByKey().mapValues(lambda x: list(x))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f4a291c",
   "metadata": {
    "id": "9f4a291c",
    "outputId": "0363bcb1-3e0b-4be1-8fd2-dc9a79a83430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 2), 1),\n",
       " ((6, 8), 13),\n",
       " ((1, 2), 9),\n",
       " ((2, 3), 2),\n",
       " ((3, 4), 2),\n",
       " ((2, 2), 2),\n",
       " ((1, 3), 1),\n",
       " ((3, 2), 3)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map.mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ece2a0",
   "metadata": {
    "id": "e1ece2a0"
   },
   "source": [
    "## Natural Join Using Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310be77e",
   "metadata": {
    "id": "310be77e"
   },
   "source": [
    "The natural join will keep the rows that matches the values in the common column for both tables. To perform natural join we will have to keep track of from which table the value came from. If the values for the same key are from different tables we need to form pairs of those values along with key to get a single row of the output. Join can explode the number of rows as we have to form each and every possible combination of the values for both tables.\n",
    "\n",
    "- **Map Function**: For two relations `Table 1(A, B)` and `Table 2(B, C)` the map function will create key-value pairs of form `b`: `[(T1, a)]` for table 1 where T1 represents the fact that the value `a` came from table 1, for table 2 key-value pairs will be of the form `b`: `[(T2, c)]`. \n",
    "\n",
    "- **Reduce Function**: For a given key `b` construct all possible combinations for the values where one value is from table `T1` and the other value is from table `T2`. The output will consist of key-value pairs of form `b: [(a, c)]` which represent one row `a, b, c` for the output table.\n",
    "\n",
    "For an example lets consider joining `Table 1` and `Table 2`, where `B` is the common column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sjy27-31QfAK",
   "metadata": {
    "id": "sjy27-31QfAK"
   },
   "outputs": [],
   "source": [
    "spark.stop() # if there is a session object named spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "Bum5C3Cx2aHG",
   "metadata": {
    "id": "Bum5C3Cx2aHG"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Relational-Algebra\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ea6701e",
   "metadata": {
    "id": "0ea6701e"
   },
   "outputs": [],
   "source": [
    "data1 = [(1, 2), (2, 3), (5, 6), (6, 1), (6, 3), (7, 6)]\n",
    "data2 = [(2, 3), (4, 4), (6, 1), (9, 8), (3, 4), (2, 1)]\n",
    "cols1 = ['A', 'B'] \n",
    "cols2 = ['B', 'C'] \n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fc7667c",
   "metadata": {
    "id": "9fc7667c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  2|  3|\n",
      "|  5|  6|\n",
      "|  6|  1|\n",
      "|  6|  3|\n",
      "|  7|  6|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd1, schema=cols1).show()\n",
    "# spark.createDataFrame(rdd2, schema=cols2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9c5a0b44",
   "metadata": {
    "id": "9c5a0b44"
   },
   "outputs": [],
   "source": [
    "+---+---+   +---+---+\n",
    "|  A|  B|   |  B|  C|\n",
    "+---+---+   +---+---+\n",
    "|  1|  2|   |  2|  3|\n",
    "|  2|  3|   |  4|  4|\n",
    "|  5|  6|   |  6|  1|\n",
    "|  6|  1|   |  9|  8|\n",
    "|  6|  3|   |  3|  4|\n",
    "|  7|  6|   |  2|  1|\n",
    "+---+---+   +---+---+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dDdEYDJv3QE7",
   "metadata": {
    "id": "dDdEYDJv3QE7"
   },
   "source": [
    "The data after applying the map function and grouping at the map workers will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "998becc5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "998becc5",
    "outputId": "36366ffc-1510-4b73-e974-d35124906ce3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, ('T1', 1)),\n",
       " (3, ('T1', 2)),\n",
       " (6, ('T1', 5)),\n",
       " (1, ('T1', 6)),\n",
       " (3, ('T1', 6)),\n",
       " (6, ('T1', 7))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1_map = rdd1.map(lambda x: (x[1], ('T1', x[0])))\n",
    "rdd1_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "jv7RKHn93Gqs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jv7RKHn93Gqs",
    "outputId": "2df227be-6fe3-444d-ba80-db39dfc071d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, ('T2', 3)),\n",
       " (4, ('T2', 4)),\n",
       " (6, ('T2', 1)),\n",
       " (9, ('T2', 8)),\n",
       " (3, ('T2', 4)),\n",
       " (2, ('T2', 1))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2_map = rdd2.map(lambda x: (x[0], ('T2', x[1])))\n",
    "rdd2_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uSEvSHyZ3pXu",
   "metadata": {
    "id": "uSEvSHyZ3pXu"
   },
   "source": [
    "Next we add the datasets together, using the `union` function. (or you can use just `+`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "mJTG4Ezr3bJt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJTG4Ezr3bJt",
    "outputId": "560d9e0e-5117-479e-97c7-9a2356b607d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, ('T1', 1)),\n",
       " (3, ('T1', 2)),\n",
       " (6, ('T1', 5)),\n",
       " (1, ('T1', 6)),\n",
       " (3, ('T1', 6)),\n",
       " (6, ('T1', 7)),\n",
       " (2, ('T2', 3)),\n",
       " (4, ('T2', 4)),\n",
       " (6, ('T2', 1)),\n",
       " (9, ('T2', 8)),\n",
       " (3, ('T2', 4)),\n",
       " (2, ('T2', 1))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd1_map.union(rdd2_map)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x5-z9un-4P0s",
   "metadata": {
    "id": "x5-z9un-4P0s"
   },
   "source": [
    "Until now, we've been working on mapping, next, we will use `groupByKey` to reduce, which sends items with the same key to the same reducer and it collects records with the same key into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "pSUPxmmv3H-V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSUPxmmv3H-V",
    "outputId": "7763c787-7408-40e0-c12e-4337835ab51e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [('T1', 6)]),\n",
       " (9, [('T2', 8)]),\n",
       " (2, [('T1', 1), ('T2', 3), ('T2', 1)]),\n",
       " (3, [('T1', 2), ('T1', 6), ('T2', 4)]),\n",
       " (4, [('T2', 4)]),\n",
       " (6, [('T1', 5), ('T1', 7), ('T2', 1)])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_grouped = (rdd1_map.union(rdd2_map)).groupByKey().mapValues(lambda x: list(x))\n",
    "rdd_grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RJzpimpo4zoL",
   "metadata": {
    "id": "RJzpimpo4zoL"
   },
   "source": [
    "After we map a function that will create a row by taking one value from table T1 and other one from T2. If there are only values from T1 or T2 in the values list that won’t constitute a row in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "en0PMfcUC-aC",
   "metadata": {
    "id": "en0PMfcUC-aC"
   },
   "outputs": [],
   "source": [
    "def cross_case(seq):\n",
    "  left, right = [], []\n",
    "  for row in seq:\n",
    "    if row[0] == 'T1':\n",
    "      left.append(row)\n",
    "    elif row[0] == 'T2':\n",
    "      right.append(row)\n",
    "\n",
    "  return [(v, w) for v in left for w in right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "yhQZsSuA5mPj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhQZsSuA5mPj",
    "outputId": "aad9d583-d023-4719-f77f-b9ee2fd3d728"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, []),\n",
       " (9, []),\n",
       " (2, [(('T1', 1), ('T2', 3)), (('T1', 1), ('T2', 1))]),\n",
       " (3, [(('T1', 2), ('T2', 4)), (('T1', 6), ('T2', 4))]),\n",
       " (4, []),\n",
       " (6, [(('T1', 5), ('T2', 1)), (('T1', 7), ('T2', 1))])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If look at B column then 2,3,6 have values in both tables\n",
    "# However, Issue is Values are being repeated below\n",
    "result = rdd_grouped.mapValues(lambda x: cross_case(x))\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UsyK801TGnVE",
   "metadata": {
    "id": "UsyK801TGnVE"
   },
   "source": [
    "This is the result of joining two tables. \n",
    "\n",
    "As we need to keep context from which table a value came from, we can’t get rid of the data that needs to be sent across the workers for application of reduce task, this operation also becomes costly as compared to others we discussed so far. The fact that for each list of values we need to create pairs also plays a major factor in the computation cost associated with this operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oaRn5XagMd1Q",
   "metadata": {
    "id": "oaRn5XagMd1Q"
   },
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ylpUQFfdWCZk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylpUQFfdWCZk",
    "outputId": "b30fa62c-38e8-4744-ff27-ab1b9f52fcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix([[ 1, 12,  5,  2],\n",
      "        [33, 27, 60,  5],\n",
      "        [ 0, 28,  0,  7],\n",
      "        [ 4, 16, 20,  0],\n",
      "        [13, 42, 23, 10]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "m = np.matrix(\n",
    "    [ \n",
    "        [1, 2, 0],\n",
    "        [3, 0, 5],\n",
    "        [0, 7, 0],\n",
    "        [4, 0, 0],\n",
    "        [1, 8, 2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "n = np.matrix(\n",
    "    [\n",
    "        [1, 4, 5, 0],\n",
    "        [0, 4, 0, 1],\n",
    "        [6, 3, 9, 1],\n",
    "    ]\n",
    ")\n",
    "shape_m = m.shape\n",
    "shape_n = n.shape\n",
    "\n",
    "m_n = m * n\n",
    "pprint(m_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "owFiGdmi4sBF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owFiGdmi4sBF",
    "outputId": "af9dc34b-cb73-48a7-a373-5dc2aa475bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1],\n",
      " [0, 1, 12],\n",
      " [0, 2, 5],\n",
      " [0, 3, 2],\n",
      " [1, 0, 33],\n",
      " [1, 1, 27],\n",
      " [1, 2, 60],\n",
      " [1, 3, 5],\n",
      " [2, 1, 28],\n",
      " [2, 3, 7],\n",
      " [3, 0, 4],\n",
      " [3, 1, 16],\n",
      " [3, 2, 20],\n",
      " [4, 0, 13],\n",
      " [4, 1, 42],\n",
      " [4, 2, 23],\n",
      " [4, 3, 10]]\n"
     ]
    }
   ],
   "source": [
    "m_n = [[i, j, m_n[i,j]] for i in range(m_n.shape[0]) for j in range(m_n.shape[1]) if m_n[i,j] != 0]\n",
    "pprint(m_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95d28b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M Shape:  (5, 3)\n",
      "N Shape:  (3, 4)\n"
     ]
    }
   ],
   "source": [
    "print('M Shape: ', shape_m)\n",
    "print('N Shape: ', shape_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ImTO6YJK4_uE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImTO6YJK4_uE",
    "outputId": "dddc6320-e6ff-41a7-987d-1dde1f317ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "[[0, 0, 1],\n",
      " [0, 1, 2],\n",
      " [1, 0, 3],\n",
      " [1, 2, 5],\n",
      " [2, 1, 7],\n",
      " [3, 0, 4],\n",
      " [4, 0, 1],\n",
      " [4, 1, 8],\n",
      " [4, 2, 2]]\n",
      "N\n",
      "[[0, 0, 1],\n",
      " [0, 1, 4],\n",
      " [0, 2, 5],\n",
      " [1, 1, 4],\n",
      " [1, 3, 1],\n",
      " [2, 0, 6],\n",
      " [2, 1, 3],\n",
      " [2, 2, 9],\n",
      " [2, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Representing matrices in the form of i, j , m[i][j] and i, j, n[i][j]\n",
    "m = [[i, j, m[i,j]] for i in range(m.shape[0]) for j in range(m.shape[1]) if m[i,j] != 0]\n",
    "\n",
    "n = [[i, j, n[i,j]] for i in range(n.shape[0]) for j in range(n.shape[1]) if n[i, j] != 0]\n",
    "\n",
    "print('M')\n",
    "pprint(m)\n",
    "print('N')\n",
    "pprint(n)\n",
    "\n",
    "# Without the 0s\n",
    "\n",
    "# [Row, Coulumn, Value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "AqWQf1V55TV6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqWQf1V55TV6",
    "outputId": "e1aee605-df52-49f8-a649-1b8a451e323e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {0: [(0, 1), (1, 3), (3, 4), (4, 1)],\n",
      "             1: [(0, 2), (2, 7), (4, 8)],\n",
      "             2: [(1, 5), (4, 2)]})\n",
      "defaultdict(<class 'list'>,\n",
      "            {0: [(0, 1), (1, 4), (2, 5)],\n",
      "             1: [(1, 4), (3, 1)],\n",
      "             2: [(0, 6), (1, 3), (2, 9), (3, 1)]})\n"
     ]
    }
   ],
   "source": [
    "# create key value pairs\n",
    "# key = j, values = [mij, ...]\n",
    "\n",
    "ma = defaultdict(list)\n",
    "for j in range(len(m)):\n",
    "    ma[m[j][1]].append((m[j][0], m[j][2])) # This way Key is column and list have row and value\n",
    "    \n",
    "# key = j, value = njk\n",
    "na = defaultdict(list)\n",
    "for j in range(len(n)):\n",
    "    na[n[j][0]].append((n[j][1], n[j][2])) # This way Key is row and list have column and value\n",
    "\n",
    "    \n",
    "    \n",
    "# Because R1xC1 * R2xC2\n",
    "# If C1 == R2, the new matrix shape is R1xC2. So imagine those two to act just like join column in inner join\n",
    "pprint(ma)\n",
    "pprint(na)\n",
    "\n",
    "# {Column: [(Row, Value)]} for ma\n",
    "# {Row: [(Column, Value)]} for na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34lqj9Hy5VvD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34lqj9Hy5VvD",
    "outputId": "9550e8fb-6919-419e-950b-3425ab653bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {(0, 0): [1],\n",
      "             (0, 1): [4, 8],\n",
      "             (0, 2): [5],\n",
      "             (0, 3): [2],\n",
      "             (1, 0): [3, 30],\n",
      "             (1, 1): [12, 15],\n",
      "             (1, 2): [15, 45],\n",
      "             (1, 3): [5],\n",
      "             (2, 1): [28],\n",
      "             (2, 3): [7],\n",
      "             (3, 0): [4],\n",
      "             (3, 1): [16],\n",
      "             (3, 2): [20],\n",
      "             (4, 0): [1, 12],\n",
      "             (4, 1): [4, 32, 6],\n",
      "             (4, 2): [5, 18],\n",
      "             (4, 3): [8, 2]})\n"
     ]
    }
   ],
   "source": [
    "# Reduce keys for each possible j value\n",
    "# Group by keys from ma, na\n",
    "# For each j value we will take each i, mij value from ma and multiply it by k, mjk\n",
    "# The key will now be this (i, k) value\n",
    "\n",
    "\n",
    "\n",
    "op = defaultdict(list)\n",
    "for j in range(shape_m[1]): # Couldve done shape_n[0] cause both are 3 and looking at # keys for ma and na dicts\n",
    "    if j in ma and j in na:\n",
    "        for mi in ma[j]: # For all list items(tuples) for key 0,1,2 in ma\n",
    "            for ni in na[j]: # For all list items(tuples) for key 0,1,2 in na\n",
    "                i = mi[0] # Picking Row from ma\n",
    "                k = ni[0] # picking column from na\n",
    "                op[(i,k)].append(mi[1] * ni[1]) # On Row Col position inserting the values\n",
    "pprint(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38ovdWlS5dBL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38ovdWlS5dBL",
    "outputId": "f8c87eb5-123c-4a9b-d4a4-66cd69c78cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1],\n",
      " [0, 1, 12],\n",
      " [0, 2, 5],\n",
      " [0, 3, 2],\n",
      " [1, 0, 33],\n",
      " [1, 1, 27],\n",
      " [1, 2, 60],\n",
      " [1, 3, 5],\n",
      " [2, 1, 28],\n",
      " [2, 3, 7],\n",
      " [3, 0, 4],\n",
      " [3, 1, 16],\n",
      " [3, 2, 20],\n",
      " [4, 0, 13],\n",
      " [4, 1, 42],\n",
      " [4, 2, 23],\n",
      " [4, 3, 10]]\n"
     ]
    }
   ],
   "source": [
    "# Group by the keys again and sum the values to get the final result which is the multiplication of m and n\n",
    "ans = list()\n",
    "for k, v in op.items():\n",
    "    ans.append([k[0], k[1], sum(v)])\n",
    "pprint(sorted(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y9qs2xQy7los",
   "metadata": {
    "id": "y9qs2xQy7los"
   },
   "source": [
    "Above, you saw how to code matrix calculation using just usual numpy and pythonic code (list comprehension). Next using this knowledge, try to convert it to PySpark code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb35fe",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "GNLCmbQ_5nQZ",
   "metadata": {
    "id": "GNLCmbQ_5nQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "[(0, 0, 1),\n",
      " (0, 1, 2),\n",
      " (1, 0, 3),\n",
      " (1, 2, 5),\n",
      " (2, 1, 7),\n",
      " (3, 0, 4),\n",
      " (4, 0, 1),\n",
      " (4, 1, 8),\n",
      " (4, 2, 2)]\n",
      "N\n",
      "[(0, 0, 1),\n",
      " (0, 1, 4),\n",
      " (0, 2, 5),\n",
      " (1, 1, 4),\n",
      " (1, 3, 1),\n",
      " (2, 0, 6),\n",
      " (2, 1, 3),\n",
      " (2, 2, 9),\n",
      " (2, 3, 1)]\n"
     ]
    }
   ],
   "source": [
    "# CODE HERE\n",
    "m = np.matrix(\n",
    "    [ \n",
    "        [1, 2, 0],\n",
    "        [3, 0, 5],\n",
    "        [0, 7, 0],\n",
    "        [4, 0, 0],\n",
    "        [1, 8, 2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "n = np.matrix(\n",
    "    [\n",
    "        [1, 4, 5, 0],\n",
    "        [0, 4, 0, 1],\n",
    "        [6, 3, 9, 1],\n",
    "    ]\n",
    ")\n",
    "shape_m = m.shape\n",
    "shape_n = n.shape\n",
    "\n",
    "m_n = m * n\n",
    "\n",
    "\n",
    "\n",
    "# Representing matrices in the form of i, j , m[i][j] and i, j, n[i][j]\n",
    "M = [(i, j, m[i,j]) for i in range(m.shape[0]) for j in range(m.shape[1]) if m[i,j] != 0]\n",
    "\n",
    "N = [(i, j, n[i,j]) for i in range(n.shape[0]) for j in range(n.shape[1]) if n[i, j] != 0]\n",
    "\n",
    "print('M')\n",
    "pprint(M)\n",
    "print('N')\n",
    "pprint(N)\n",
    "\n",
    "# Without the 0s\n",
    "\n",
    "# [Row, Coulumn, Value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a821b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = sc.parallelize(M)\n",
    "N = sc.parallelize(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "02733140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0, 1, 'M')),\n",
       " (1, (0, 2, 'M')),\n",
       " (0, (1, 3, 'M')),\n",
       " (2, (1, 5, 'M')),\n",
       " (1, (2, 7, 'M')),\n",
       " (0, (3, 4, 'M')),\n",
       " (0, (4, 1, 'M')),\n",
       " (1, (4, 8, 'M')),\n",
       " (2, (4, 2, 'M'))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.map(lambda x: (x[1], (x[0], x[2], 'M'))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1eccdad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "[(0, [(0, 1, 'M'), (1, 3, 'M'), (3, 4, 'M'), (4, 1, 'M')]),\n",
      " (1, [(0, 2, 'M'), (2, 7, 'M'), (4, 8, 'M')]),\n",
      " (2, [(1, 5, 'M'), (4, 2, 'M')])]\n",
      "N\n",
      "[(0, [(0, 1, 'N'), (1, 4, 'N'), (2, 5, 'N')]),\n",
      " (1, [(1, 4, 'N'), (3, 1, 'N')]),\n",
      " (2, [(0, 6, 'N'), (1, 3, 'N'), (2, 9, 'N'), (3, 1, 'N')])]\n"
     ]
    }
   ],
   "source": [
    "print('M')\n",
    "NewM = M.map(lambda x: (x[1], (x[0], x[2], 'M'))).groupByKey().mapValues(lambda x: list(x))\n",
    "pprint(NewM.collect())\n",
    "print('N')\n",
    "NewN = N.map(lambda x: (x[0], (x[1], x[2], 'N'))).groupByKey().mapValues(lambda x: list(x))\n",
    "pprint(NewN.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a7cbdbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [(0, 1, 'M'),\n",
       "   (1, 3, 'M'),\n",
       "   (3, 4, 'M'),\n",
       "   (4, 1, 'M'),\n",
       "   (0, 1, 'N'),\n",
       "   (1, 4, 'N'),\n",
       "   (2, 5, 'N')]),\n",
       " (1, [(0, 2, 'M'), (2, 7, 'M'), (4, 8, 'M'), (1, 4, 'N'), (3, 1, 'N')]),\n",
       " (2,\n",
       "  [(1, 5, 'M'),\n",
       "   (4, 2, 'M'),\n",
       "   (0, 6, 'N'),\n",
       "   (1, 3, 'N'),\n",
       "   (2, 9, 'N'),\n",
       "   (3, 1, 'N')])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = (NewM + NewN).reduceByKey(lambda x, y: (x + y))\n",
    "grouped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f35cf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplyMatrix(row):\n",
    "#     MTable, NTable = [], []\n",
    "#     for e in row:\n",
    "#         if e[2] == \"M\":\n",
    "    return [(100,200),(200,300),(400,500)]\n",
    "       \n",
    "    \n",
    "# left, right = [], []\n",
    "# for row in seq:\n",
    "# if row[0] == 'T1':\n",
    "#   left.append(row)\n",
    "# elif row[0] == 'T2':\n",
    "#   right.append(row)\n",
    "\n",
    "# return [(v, w) for v in left for w in right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf3b4229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(100, 200), (200, 300), (400, 500)],\n",
       " [(100, 200), (200, 300), (400, 500)],\n",
       " [(100, 200), (200, 300), (400, 500)]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.map(lambda x: multiplyMatrix(x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1aec86de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [(0, 1), (1, 3), (3, 4), (4, 1)]),\n",
      " (0, [(0, 1), (1, 4), (2, 5)]),\n",
      " (1, [(0, 2), (2, 7), (4, 8)]),\n",
      " (1, [(1, 4), (3, 1)]),\n",
      " (2, [(1, 5), (4, 2)]),\n",
      " (2, [(0, 6), (1, 3), (2, 9), (3, 1)])]\n",
      "[(0, [0, 1]), (0, [0, 1]), (1, [0, 2]), (1, [1, 4]), (2, [1, 5]), (2, [0, 6])]\n"
     ]
    }
   ],
   "source": [
    "pprint((NewM + NewN).collect())\n",
    "pprint((NewM + NewN).map(lambda x: (x[0], list(x[1][0]))).collect())\n",
    "# .collect()# .reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tbITMiwYF8C6",
   "metadata": {
    "id": "tbITMiwYF8C6"
   },
   "source": [
    "Stop all previous sessions/contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tn-9MSpOGA5I",
   "metadata": {
    "id": "tn-9MSpOGA5I"
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iIuWwjW-GxzI",
   "metadata": {
    "id": "iIuWwjW-GxzI"
   },
   "source": [
    "Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bnnb_B_e8EnO",
   "metadata": {
    "id": "bnnb_B_e8EnO"
   },
   "outputs": [],
   "source": [
    "# code here \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Matrix-Multiplication\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ThrLVurOI6",
   "metadata": {
    "id": "20ThrLVurOI6"
   },
   "source": [
    "[Matrix Multiplication Example](https://raw.githubusercontent.com/tnurbek/ds702/main/Lab2/1_Go9f2bN64Y4wrFLhd0Cl3w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VRPtEmDdIPMF",
   "metadata": {
    "id": "VRPtEmDdIPMF"
   },
   "outputs": [],
   "source": [
    "matrix1 = [[1, 2, 0], [3, 0, 5], [0, 7, 0], [4, 0, 0], [1, 8, 2]]\n",
    "matrix2 = [[1, 4, 5, 0], [0, 4, 0, 1], [6, 3, 9, 1]]\n",
    "\n",
    "matrix1 = [[i, j, matrix1[i][j]] for i in range(len(matrix1)) for j in range(len(matrix1[0])) if matrix1[i][j] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4yVzjbugaa",
   "metadata": {
    "id": "4b4yVzjbugaa"
   },
   "outputs": [],
   "source": [
    "matrix1 = sc.parallelize(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t7AmBMC5ux95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7AmBMC5ux95",
    "outputId": "a5ee424d-005f-4d12-bd96-e6a64240063c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 1),\n",
       " ((0, 1), 2),\n",
       " ((1, 0), 3),\n",
       " ((1, 2), 5),\n",
       " ((2, 1), 7),\n",
       " ((3, 0), 4),\n",
       " ((4, 0), 1),\n",
       " ((4, 1), 8),\n",
       " ((4, 2), 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1.map(lambda x: ((x[0], x[1]), x[2])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UYw-lxpUu7jv",
   "metadata": {
    "id": "UYw-lxpUu7jv"
   },
   "outputs": [],
   "source": [
    "# code here..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e588ff42",
    "70f367f0",
    "3836b60d",
    "4d8ac09e",
    "d8038bb7",
    "d3502614"
   ],
   "name": "Lab2_Relational_Algebra_Operations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
