{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_FfWa1Cyx0e"
   },
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MIKf15cy1Zr"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Spark Streaming\n",
    "https://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "\n",
    "PySpark\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAvJlsmDy_cd"
   },
   "source": [
    "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. \n",
    "\n",
    "Data can be ingested from many sources like `Kafka, Kinesis, or TCP sockets`, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. \n",
    "\n",
    "Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\n",
    "\n",
    "![Spark Streaming Architecture](https://raw.githubusercontent.com/tnurbek/ds702/main/Lab7/streaming-arch.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9cy1wVH0Iao"
   },
   "source": [
    "Spark Streaming provides a high-level abstraction called discretized stream or `DStream`, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZrRPijv0rQD"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEq5Worw0xDl",
    "outputId": "64035f46-e1e4-4913-cc07-71cb2cda5fc1"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8646l090kou"
   },
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Goi5UakY0a4p"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 02:57:52 WARN Utils: Your hostname, umar resolves to a loopback address: 127.0.1.1; using 10.127.80.168 instead (on interface wlp2s0)\n",
      "22/03/16 02:57:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/umar/Desktop/krypton/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/16 02:57:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Stream').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will work with structured streaming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('id', IntegerType(), True), \n",
    "        StructField('name', StringType(), True), \n",
    "        StructField('age', IntegerType(), True), \n",
    "        StructField('profession', StringType(), True), \n",
    "        StructField('city', StringType(), True), \n",
    "        StructField('salary', DoubleType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load our data into a streaming DataFrame by using the `readStream`.\n",
    "\n",
    "\n",
    "We can also check status of our streaming with the `isStreaming` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.readStream.format('csv').schema(schema).option('header', True).option('maxFilesPerTrigger', 1).load(\"./Lab7_Extra/stream/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we created streaming DataFrame. Next, we will simulate streaming. Instead of streaming data as it comes in, we will copy each of our csv files one at a time to our path that we specified in “readStream” above in the code. That’s why we are also setting “maxFilesPerTrigger” option to 1, which tells us only a single csv file will be streamed at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- profession: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will apply some transformations which will show us the number of people from each profession and also average salaries of professions with descending order in a DataFrame that will be updated with every new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_salaries = data.groupBy('profession').agg(\n",
    "    (avg('salary').alias('average_salary')), \n",
    "    (count('profession').alias('count'))\n",
    ").sort(desc('average_salary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for Streaming except one last point; we need to specify a `format()` for streaming to a destination \n",
    "\n",
    "and `outputMode()` for the determination of the data to be written into a streaming sink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used formats are console, kafka, parquet and memory. We will use the console option as format so we can follow our streaming results from terminal.\n",
    "\n",
    "We have three options for outputMode():\n",
    "\n",
    "- `append`: Only new rows will be written to the sink.\n",
    "- `complete`: All rows will be written to the sink, every time there are updates.\n",
    "- `update`: Only the updated rows will be written to the sink, every time there are updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 02:58:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-09e9336c-dc64-422f-8c19-056deb712782. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/03/16 02:58:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = average_salaries.writeStream.format('console').outputMode('complete').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fake = Faker() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_data = [] \n",
    "id = 100\n",
    "professions = ['doctor', 'teacher', 'police', 'lawyer', 'musician', 'ml engineer', 'designer', 'software engineer', 'journalist', 'fireman', 'singer']\n",
    "\n",
    "for i in range (6): \n",
    "    stream_data = []\n",
    "    for j in range(20):\n",
    "        arr = []\n",
    "\n",
    "        # id \n",
    "        arr.append(id)\n",
    "        id += 1\n",
    "\n",
    "        # name \n",
    "        arr.append(fake.name())\n",
    "\n",
    "        # age \n",
    "        arr.append(np.random.randint(18, 72))\n",
    "\n",
    "        # profession \n",
    "        arr.append(np.random.choice(professions))\n",
    "\n",
    "        # city \n",
    "        arr.append(fake.city())\n",
    "\n",
    "        # salary \n",
    "        arr.append(np.random.randint(1000, 12000))\n",
    "\n",
    "        stream_data.append(arr)\n",
    "    customers_data.append(stream_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(customers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[100, 'Julie Collins', 24, 'journalist', 'North Betty', 9887],\n",
       "  [101, 'Stephanie Burton', 33, 'fireman', 'Ashleyfurt', 6431],\n",
       "  [102, 'Tracie Hardin', 28, 'software engineer', 'North Nancyton', 5785],\n",
       "  [103, 'Theresa Myers', 49, 'doctor', 'North Melissahaven', 4876],\n",
       "  [104, 'Connie Petty', 69, 'ml engineer', 'New Alyssa', 6995],\n",
       "  [105, 'Brian Garza', 26, 'police', 'New Rodney', 5231],\n",
       "  [106, 'Carrie Blanchard', 34, 'designer', 'North Heatherside', 4686],\n",
       "  [107, 'Morgan Harris', 35, 'ml engineer', 'Longstad', 5740],\n",
       "  [108, 'Ronald Bradford', 52, 'teacher', 'Christophershire', 3410],\n",
       "  [109, 'Kenneth Reed', 41, 'fireman', 'Hoborough', 1191],\n",
       "  [110, 'Joshua Smith', 43, 'singer', 'Port James', 11051],\n",
       "  [111, 'Emily Roberts', 39, 'journalist', 'West Claireburgh', 9515],\n",
       "  [112, 'Paul Sanchez', 55, 'musician', 'New Juliestad', 9177],\n",
       "  [113, 'Amy Burns', 69, 'software engineer', 'Lake Karenbury', 1275],\n",
       "  [114, 'Katherine Howard', 45, 'singer', 'Hansonmouth', 8229],\n",
       "  [115, 'Brooke Houston', 69, 'fireman', 'Timothystad', 6965],\n",
       "  [116, 'Frank Moore', 19, 'software engineer', 'South Kimberlyberg', 7830],\n",
       "  [117, 'John Shaw', 43, 'software engineer', 'Madisonburgh', 6972],\n",
       "  [118, 'Rita Allen', 64, 'lawyer', 'Lopezshire', 4098],\n",
       "  [119, 'Mark Dean', 50, 'software engineer', 'Port Jacobton', 7296]],\n",
       " [[120, 'Alexander Huynh', 25, 'doctor', 'East Michael', 6948],\n",
       "  [121, 'Casey Jimenez', 40, 'software engineer', 'Ericborough', 3623],\n",
       "  [122, 'Angela Velazquez', 57, 'journalist', 'Johnsonbury', 7920],\n",
       "  [123, 'Brian Wallace', 44, 'doctor', 'South Kimberlyport', 6325],\n",
       "  [124, 'Shelly Shannon', 29, 'doctor', 'Valerieberg', 2567],\n",
       "  [125, 'Janet Park', 18, 'doctor', 'Helenmouth', 3255],\n",
       "  [126, 'Diane Roberts', 24, 'singer', 'South Tylerview', 3094],\n",
       "  [127, 'Theresa Le', 59, 'fireman', 'Charlesland', 4447],\n",
       "  [128, 'Anita King', 45, 'software engineer', 'West Dillon', 8096],\n",
       "  [129, 'Samantha Nichols', 43, 'ml engineer', 'Clarkborough', 4431],\n",
       "  [130, 'Dana Reynolds', 39, 'police', 'East Sherryville', 4609],\n",
       "  [131, 'Sheri Peterson', 48, 'designer', 'North Ryan', 4912],\n",
       "  [132, 'Alexandria Richards', 53, 'designer', 'North Adamhaven', 2608],\n",
       "  [133, 'Sydney Fitzpatrick', 64, 'software engineer', 'Carriemouth', 4124],\n",
       "  [134, 'Mr. Scott Rodriguez II', 32, 'lawyer', 'East James', 11605],\n",
       "  [135, 'Bryan Cardenas', 28, 'police', 'Brandonview', 10812],\n",
       "  [136, 'Daryl Clark', 56, 'singer', 'Richardborough', 1944],\n",
       "  [137, 'Bradley Jackson', 56, 'lawyer', 'East Destiny', 11280],\n",
       "  [138, 'Tammy Morton', 69, 'designer', 'Clintonville', 5467],\n",
       "  [139, 'Kevin Bruce', 50, 'doctor', 'Sullivanfurt', 3206]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put first three files into `stream/` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------------+-----------------+-----+\n",
      "|       profession|   average_salary|count|\n",
      "+-----------------+-----------------+-----+\n",
      "|       journalist|           9701.0|    2|\n",
      "|           singer|           9640.0|    2|\n",
      "|         musician|           9177.0|    1|\n",
      "|      ml engineer|           6367.5|    2|\n",
      "|software engineer|           5831.6|    5|\n",
      "|           police|           5231.0|    1|\n",
      "|           doctor|           4876.0|    1|\n",
      "|          fireman|4862.333333333333|    3|\n",
      "|         designer|           4686.0|    1|\n",
      "|           lawyer|           4098.0|    1|\n",
      "|          teacher|           3410.0|    1|\n",
      "+-----------------+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------+-----------------+-----+\n",
      "|       profession|   average_salary|count|\n",
      "+-----------------+-----------------+-----+\n",
      "|         musician|           9177.0|    1|\n",
      "|       journalist|9107.333333333334|    3|\n",
      "|           lawyer|8994.333333333334|    3|\n",
      "|           police|           6884.0|    3|\n",
      "|           singer|           6079.5|    4|\n",
      "|      ml engineer|           5722.0|    3|\n",
      "|software engineer|         5625.125|    8|\n",
      "|          fireman|           4758.5|    4|\n",
      "|           doctor|           4529.5|    6|\n",
      "|         designer|          4418.25|    4|\n",
      "|          teacher|           3410.0|    1|\n",
      "+-----------------+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------+-----------------+-----+\n",
      "|       profession|   average_salary|count|\n",
      "+-----------------+-----------------+-----+\n",
      "|         musician|           8908.0|    3|\n",
      "|           lawyer|           8099.6|    5|\n",
      "|           police|           7868.0|    4|\n",
      "|       journalist|           7366.6|    5|\n",
      "|      ml engineer|          6581.75|    4|\n",
      "|           singer|           6227.4|   10|\n",
      "|         designer|6159.833333333333|    6|\n",
      "|software engineer|         5625.125|    8|\n",
      "|          teacher|           5294.5|    2|\n",
      "|           doctor|5187.111111111111|    9|\n",
      "|          fireman|           4758.5|    4|\n",
      "+-----------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(customers_data): \n",
    "    if i < 3: \n",
    "        df = pd.DataFrame(elem, columns=\"id name age profession city salary\".split(\" \")) \n",
    "        filename = f'./Lab7_Extra/stream/file{i}.csv' \n",
    "        df.to_csv(filename, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you noticed, streaming takes one file at a time and processes it. Let's put another file into `stream/` folder and see aggregation result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>profession</th>\n",
       "      <th>city</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>Michele Gibbs</td>\n",
       "      <td>55</td>\n",
       "      <td>teacher</td>\n",
       "      <td>Lake Jenniferfort</td>\n",
       "      <td>7179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>Michele Long</td>\n",
       "      <td>39</td>\n",
       "      <td>police</td>\n",
       "      <td>Port Joy</td>\n",
       "      <td>10820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142</td>\n",
       "      <td>Jordan Moore</td>\n",
       "      <td>58</td>\n",
       "      <td>doctor</td>\n",
       "      <td>Stevenchester</td>\n",
       "      <td>8115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143</td>\n",
       "      <td>Valerie Davis</td>\n",
       "      <td>50</td>\n",
       "      <td>musician</td>\n",
       "      <td>Lake Derrick</td>\n",
       "      <td>10770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144</td>\n",
       "      <td>Jessica Chambers</td>\n",
       "      <td>45</td>\n",
       "      <td>journalist</td>\n",
       "      <td>Robinmouth</td>\n",
       "      <td>2819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id              name  age  profession               city  salary\n",
       "0  140     Michele Gibbs   55     teacher  Lake Jenniferfort    7179\n",
       "1  141      Michele Long   39      police           Port Joy   10820\n",
       "2  142      Jordan Moore   58      doctor      Stevenchester    8115\n",
       "3  143     Valerie Davis   50    musician       Lake Derrick   10770\n",
       "4  144  Jessica Chambers   45  journalist         Robinmouth    2819"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./Lab7_Extra/stream/file2.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------+-----------------+-----+\n",
      "|       profession|   average_salary|count|\n",
      "+-----------------+-----------------+-----+\n",
      "|           police|           7868.0|    4|\n",
      "|         musician|           7636.2|    5|\n",
      "|           lawyer|7487.142857142857|    7|\n",
      "|      ml engineer|           7405.0|    5|\n",
      "|         designer|6735.777777777777|    9|\n",
      "|          teacher|           6673.2|    5|\n",
      "|       journalist|           6450.0|    6|\n",
      "|software engineer|           5636.1|   10|\n",
      "|          fireman|           5606.2|    5|\n",
      "|           singer|5535.076923076923|   13|\n",
      "|           doctor|5358.727272727273|   11|\n",
      "+-----------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(customers_data[3], columns=\"id name age profession city salary\".split(\" \"))\n",
    "filename = f'./Lab7_Extra/stream/file3.csv'\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the streaming: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab7. Streaming.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
