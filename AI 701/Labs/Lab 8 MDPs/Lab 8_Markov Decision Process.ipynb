{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement robot navigation problem through Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process is a mathematical model of sequential decision, which is used to simulate the achievable random strategy and rewards of the agent in an environment where the system state has Markov properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transitions stores transition state probability.\n",
    "\n",
    "Reward stores reward value for reaching a certain state.\n",
    "\n",
    "Gamma is the discount factor.It is used to avoid infinite returns in the loop or infinite Markov decision process.We set it to 0.9 as a test.\n",
    "\n",
    "Epsilon is the maximum error allowed in the utility of any state.We set it to 0.001 as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "Transitions = {}\n",
    "Reward = {}\n",
    "gamma = 0.9\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define file reading function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read trasitions and rewards from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    #Read transitions from file and store it to a variable.\n",
    "    with open('./data/transitions.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[0] in Transitions:\n",
    "                if row[1] in Transitions[row[0]]:\n",
    "                    Transitions[row[0]][row[1]].append((float(row[3]), row[2]))\n",
    "                else:\n",
    "                    Transitions[row[0]][row[1]] = [(float(row[3]), row[2])]\n",
    "            else:\n",
    "                Transitions[row[0]] = {row[1]:[(float(row[3]),row[2])]}\n",
    "\n",
    "    #Read rewards file and save it to a variable.\n",
    "    with open('./data/rewards.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            Reward[row[0]] = float(row[1]) if row[1] != 'None' else None\n",
    "\n",
    "read_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process contains states, actions, transition model and reward function.\n",
    "\n",
    "States represent the set of all states of the robot.\n",
    "\n",
    "Actions represent the set of actions that can be performed in this state.\n",
    "\n",
    "Transition represents the probability of transition from one state to another.\n",
    "\n",
    "Reward represents the reward value for this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess:\n",
    "    def __init__(self, transition={}, reward={}, gamma=.9):\n",
    "        #Collect all nodes from the transition models.\n",
    "        self.states = transition.keys()\n",
    "        #Initialize transition.\n",
    "        self.transition = transition\n",
    "        #Initialize reward.\n",
    "        self.reward = reward\n",
    "        #Initialize gamma.\n",
    "        self.gamma = gamma\n",
    "\n",
    "     # Reward for this state.\n",
    "    def R(self, state):\n",
    "        return self.reward[state]\n",
    "    \n",
    "    # Set of actions that can be performed in this state.\n",
    "    def actions(self, state):\n",
    "        return self.transition[state].keys()\n",
    "    \n",
    "    #For a state and an action, return a list of (probability, result-state) pairs.\n",
    "    def T(self, state, action):\n",
    "        \n",
    "        return self.transition[state][action]\n",
    "\n",
    "#Initialize the MarkovDecisionProcess object.\n",
    "mdp = MarkovDecisionProcess(transition=Transitions, reward=Reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the MDP by value iteration,the value iteration process:\n",
    "\n",
    "\n",
    "(1) Initialize V(s) for  each state s.\n",
    "\n",
    "\n",
    "(2) For each state s,update $ V(s)=R(s)+ \\gamma max_{a\\epsilon A}\\sum_{{s}'}P_{sa}({s}')V({s}') $.\n",
    "\n",
    "\n",
    "(3) Repeat step (2) until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    states = mdp.states\n",
    "    actions = mdp.actions\n",
    "    T = mdp.T\n",
    "    R = mdp.R\n",
    "\n",
    "    #Initialize value of all the states to 0 (this is k=0 case).\n",
    "    V1 = {s: 0 for s in states}\n",
    "    while True:\n",
    "        V = V1.copy()\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            #Bellman update, update the utility values.\n",
    "            V1[s] = R(s) + gamma * max([ sum([p * V[s1] for (p, s1) in T(s, a)]) for a in actions(s)])\n",
    "            #calculate maximum difference in value\n",
    "            delta = max(delta, abs(V1[s] - V[s]))\n",
    "\n",
    "        #Check for convergence, if values converged then return V.\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define calculation of the best policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given an MDP and a utility values V, determine the best policy as a mapping from state to action.\n",
    " \n",
    " For each state s,update $\\pi(s)=max(a(s),\\sum_{{s'}} P({s'})*V({s'}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(V):\n",
    "    states = mdp.states\n",
    "    actions = mdp.actions\n",
    "    pi = {}\n",
    "    for s in states:\n",
    "        pi[s] = max(actions(s), key=lambda a: expected_utility(a, s, V))\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected utility of doing a in state s, according to the MDP and V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utility(a, s, V):\n",
    "    T = mdp.T\n",
    "    return sum([p * V[s1] for (p, s1) in mdp.T(s, a)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State - Value\n",
      "(3 0)  -  0.12987274656746342\n",
      "(3 1)  -  -1.0\n",
      "(1 0)  -  0.25386699846479516\n",
      "(2 1)  -  0.48644001739269643\n",
      "(1 2)  -  0.649585681261095\n",
      "(2 0)  -  0.3447542300124158\n",
      "(3 2)  -  1.0\n",
      "(2 2)  -  0.7953620878466678\n",
      "(0 1)  -  0.3984432178350045\n",
      "(0 0)  -  0.2962883154554812\n",
      "(0 2)  -  0.5093943765842497\n",
      "\n",
      "Optimal policy is \n",
      "State - Action\n",
      "(3 0)  -  L\n",
      "(3 1)  -  EXIT\n",
      "(1 0)  -  R\n",
      "(2 1)  -  U\n",
      "(1 2)  -  R\n",
      "(2 0)  -  U\n",
      "(3 2)  -  EXIT\n",
      "(2 2)  -  R\n",
      "(0 1)  -  U\n",
      "(0 0)  -  U\n",
      "(0 2)  -  R\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Call value iteration.\n",
    "    V = value_iteration()\n",
    "    print ('State - Value')\n",
    "    for s in V:\n",
    "        print (s, ' - ' , V[s])\n",
    "    pi = best_policy(V)\n",
    "    print ('\\nOptimal policy is \\nState - Action')\n",
    "    for s in pi:\n",
    "        print (s, ' - ' , pi[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
