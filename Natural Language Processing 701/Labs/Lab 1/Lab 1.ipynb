{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1483961c",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64403baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m \n",
    "# # pip install -U nltk\n",
    "# # nltk\n",
    "# # matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d90ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1c80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd248b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def showImage(x):    \n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.axis('off')\n",
    "    img = mpimg.imread('./' + str(x) + '.PNG')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# C:\\Users\\user\\AppData\\Roaming\\nltk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd23c9",
   "metadata": {},
   "source": [
    "## Tokenizing Text and WordNet Basics\n",
    "\n",
    "1 Tokenizing text into sentences  \n",
    "2 Tokenizing sentences into words  \n",
    "3 Tokenizing sentences using regular expressions  \n",
    "4 Training a sentence tokenizer  \n",
    "5 Filtering stopwords in a tokenized sentence  \n",
    "6 Looking up Synsets for a word in WordNet  \n",
    "7 Looking up lemmas and synonyms in WordNet  \n",
    "8 Calculating WordNet Synset similarity  \n",
    "9 Discovering word collocation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67be49f",
   "metadata": {},
   "source": [
    "#### Senetence Tokenize\n",
    "\n",
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the \n",
    "nltk.tokenize.punkt module. This instance has already been trained and works well for \n",
    "many European languages. So it knows what punctuation and characters mark the end of a \n",
    "sentence and the beginning of a new sentenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"Hello World. It's good to see you. Thanks for buying this book.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac05be",
   "metadata": {},
   "source": [
    "The instance used in sent_tokenize() is actually loaded on demand from a pickle \n",
    "file. So if you're going to be tokenizing a lot of sentences, it's more efficient to load the \n",
    "PunktSentenceTokenizer class once, and call its tokenize() method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR\n",
    "\n",
    "# import nltk.data\n",
    "# tokenizer = nltk.data.load('C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt\\\\PY3\\\\english.pickle')\n",
    "# tokenizer.tokenize(para)\n",
    "# ['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e84889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('webtext')\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "sents1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sents2 = sent_tokenize(text)\n",
    "sents2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference\n",
    "\n",
    "print(sents1[678])\n",
    "print(sents2[678])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab520e",
   "metadata": {},
   "source": [
    "The default tokenizer includes the next line of dialog, while our custom tokenizer correctly thinks that \n",
    "the next line is a separate sentence. This difference is a good demonstration of why it can \n",
    "be useful to train your own sentence tokenizer, especially when your text isn't in the typical \n",
    "paragraph-sentence structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use on your own corpus\n",
    "\n",
    "with open('/usr/share/nltk_data/corpora/webtext/overheard.txt', \n",
    "encoding='ISO-8859-2') as f:\n",
    "    text = f.read()\n",
    "    sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "    sents = sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd56ccc",
   "metadata": {},
   "source": [
    "#### Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ada789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Hello World.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49881c",
   "metadata": {},
   "source": [
    "The word_tokenize() function is a wrapper function that calls tokenize() on an \n",
    "instance of the TreebankWordTokenizer class.   \n",
    "It's equivalent to the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('Hello World.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktWordTokenizer\n",
    "tokenizer = PunktWordTokenizer()\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")\n",
    "\n",
    "# Output: ['Can', \"'t\", 'is', 'a', 'contraction.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc511c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitespace Tokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dbd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a68bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffed4817",
   "metadata": {},
   "source": [
    "#### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4cda43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f403057a",
   "metadata": {},
   "source": [
    "#### Looking up Synsets for a word in WordNet\n",
    "\n",
    "WordNet is a lexical database for the English language. In other words, it's a dictionary \n",
    "designed specifically for natural language processing.  \n",
    "\n",
    "\n",
    "NLTK comes with a simple interface to look up words in WordNet. What you get is a list of \n",
    "Synset instances, which are groupings of synonymous words that express the same concept. \n",
    "Many words have only one Synset, but some have several. In this recipe, we'll explore a single \n",
    "Synset, and in the next recipe, we'll look at several in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "print(syn.name())\n",
    "print(syn.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synsets(\"Blood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7908c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " wordnet.synsets('cooking')[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d9930",
   "metadata": {},
   "outputs": [],
   "source": [
    " wordnet.synsets('Blood')[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2adbd",
   "metadata": {},
   "source": [
    "#### Working with hypernyms\n",
    "Synsets are organized in a structure similar to that of an inheritance tree. More abstract terms \n",
    "are known as hypernyms and more specific terms are hyponyms. This tree can be traced all \n",
    "the way up to a root hypernym.  \n",
    "\n",
    "Hypernyms provide a way to categorize and group words based on their similarity to each \n",
    "other. The Calculating WordNet Synset similarity recipe details the functions used to calculate \n",
    "the similarity based on the distance between two words in the hypernym tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65534926",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ccbf8",
   "metadata": {},
   "source": [
    "As you can see, reference_book is a hypernym of cookbook, but cookbook is only one of \n",
    "the many hyponyms of reference_book. And all these types of books have the same root \n",
    "hypernym, which is entity, one of the most abstract terms in the English language. You can \n",
    "trace the entire path from entity down to cookbook using the hypernym_paths() method, \n",
    "as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bba2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a6e0e",
   "metadata": {},
   "source": [
    "Noun      n  \n",
    "Adjective a  \n",
    "Adverb    r  \n",
    "Verb      v  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571aa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordnet.synsets('great'))\n",
    "len(wordnet.synsets('great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synsets('great', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353649c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordnet.synsets('great', pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23556d",
   "metadata": {},
   "source": [
    "#### Looking up lemmas and synonyms in WordNet\n",
    "\n",
    "Building on the previous recipe, we can also look up lemmas in WordNet to find synonyms \n",
    "of a word. A lemma (in linguistics), is the canonical form or morphological form of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce750bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()\n",
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d55732",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas[1].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas[0].synset() == lemmas[1].synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemma.name() for lemma in syn.lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7932df",
   "metadata": {},
   "outputs": [],
   "source": [
    " synonyms = []\n",
    "for syn in wordnet.synsets('book'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "len(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200125af",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81013d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    " len(set(synonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8812b",
   "metadata": {},
   "source": [
    "#### Antonyms\n",
    "Some lemmas also have antonyms. The word good, for example, has 27 Synsets, five \n",
    "of which have lemmas with antonyms, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gn2 = wordnet.synset('good.n.02')\n",
    "gn2.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c71efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evil = gn2.lemmas()[0].antonyms()[0]\n",
    "evil.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evil.synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58da6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga1 = wordnet.synset('good.a.01')\n",
    "ga1.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = ga1.lemmas()[0].antonyms()[0]\n",
    "bad.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70048f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad.synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59fa83b",
   "metadata": {},
   "source": [
    "The antonyms() method returns a list of lemmas. In the first case, as we can see in the \n",
    "previous code, the second Synset for good as a noun is defined as moral excellence, \n",
    "and its first antonym is evil, defined as morally wrong. In the second case, when good is \n",
    "used as an adjective to describe positive qualities, the first antonym is bad, which describes \n",
    "negative qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4471ff",
   "metadata": {},
   "source": [
    "#### Calculating WordNet Synset similarity\n",
    "Synsets are organized in a hypernym tree. This tree can be used for reasoning about \n",
    "the similarity between the Synsets it contains. The closer the two Synsets are in the tree, \n",
    "the more similar they are\n",
    "\n",
    "If you were to look at all the hyponyms of reference_book (which is the hypernym of \n",
    "cookbook), you'd see that one of them is instruction_book. This seems intuitively very \n",
    "similar to a cookbook, so let's see what WordNet similarity has to say about it with the help \n",
    "of the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "cb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c05b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "The wup_similarity method is short for Wu-Palmer Similarity, which is a scoring method \n",
    "based on how similar the word senses are and where the Synsets occur relative to each other \n",
    "in the hypernym tree. One of the core metrics used to calculate similarity is the shortest path \n",
    "distance between the two Synsets and their common hypernym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7372e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = cb.hypernyms()[0]\n",
    "cb.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.shortest_path_distance(ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c579ce3",
   "metadata": {},
   "source": [
    "So cookbook and instruction_book must be very similar, because they are only one step \n",
    "away from the same reference_book hypernym, and, therefore, only two steps away from \n",
    "each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wordnet.synsets('dog')[0]\n",
    "dog.wup_similarity(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wow, dog and cookbook are apparently 38% similar! This is because they share common \n",
    "hypernyms further up the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a02108",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dog.common_hypernyms(cb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53390194",
   "metadata": {},
   "source": [
    "#### Comparing verbs\n",
    "The previous comparisons were all between nouns, but the same can be done for verbs \n",
    "as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cook = wordnet.synset('cook.v.01')\n",
    "bake = wordnet.synset('bake.v.02')\n",
    "cook.wup_similarity(bake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bcabe",
   "metadata": {},
   "source": [
    "The previous Synsets were obviously handpicked for demonstration, and the reason is that \n",
    "the hypernym tree for verbs has a lot more breadth and a lot less depth. While most nouns \n",
    "can be traced up to the hypernym object, thereby providing a basis for similarity, many \n",
    "verbs do not share common hypernyms, making WordNet unable to calculate the similarity. \n",
    "For example, if you were to use the Synset for bake.v.01 in the previous code, instead of \n",
    "bake.v.02, the return value would be None. This is because the root hypernyms of both the \n",
    "Synsets are different, with no overlapping paths. For this reason, you also cannot calculate \n",
    "the similarity between words with different parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa5982",
   "metadata": {},
   "source": [
    "#### Path and Leacock Chordorow (LCH) similarity\n",
    "Two other similarity comparisons are the path similarity and the LCH similarity, as shown in \n",
    "the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf764b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cb.path_similarity(ib))\n",
    "\n",
    "print(cb.path_similarity(dog))\n",
    "\n",
    "print(cb.lch_similarity(ib))\n",
    "\n",
    "print(cb.lch_similarity(dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82649e88",
   "metadata": {},
   "source": [
    "### Discovering word collocations\n",
    "Collocations are two or more words that tend to appear frequently together, such as United \n",
    "States. Of course, there are many other words that can come after United, such as United \n",
    "Kingdom and United Airlines. As with many aspects of natural language processing, context \n",
    "is very important. And for collocations, context is everything!\n",
    "\n",
    "In the case of collocations, the context will be a document in the form of a list of words. \n",
    "Discovering collocations in this list of words means that we'll find common phrases that \n",
    "occur frequently throughout the text. For fun, we'll start with the script for Monty Python \n",
    "and the Holy Grail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1afb6",
   "metadata": {},
   "source": [
    "We're going to create a list of all lowercased words in the text, and then produce \n",
    "BigramCollocationFinder, which we can use to find bigrams, which are pairs of words. \n",
    "These bigrams are found using association measurement functions in the nltk.metrics\n",
    "package, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d5fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8d1bb",
   "metadata": {},
   "source": [
    "Well, that's not very useful! Let's refine it a bit by adding a word filter to remove punctuation \n",
    "and stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebf2f5",
   "metadata": {},
   "source": [
    "BigramCollocationFinder constructs two frequency distributions: one for each word, \n",
    "and another for bigrams. A frequency distribution, or FreqDist in NLTK, is basically an \n",
    "enhanced Python dictionary where the keys are what's being counted, and the values are \n",
    "the counts. Any filtering functions that are applied reduce the size of these two FreqDists\n",
    "by eliminating any words that don't pass the filter. By using a filtering function to eliminate all \n",
    "words that are one or two characters, and all English stopwords, we can get a much cleaner \n",
    "result. After filtering, the collocation finder is ready to accept a generic scoring function for \n",
    "finding collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "\n",
    "words = [w.lower() for w in webtext.words('singles.txt')]\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "tcf.apply_freq_filter(3)\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edcdd96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
