{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5eadc64",
   "metadata": {
    "id": "b5eadc64"
   },
   "source": [
    "# Lab 1. PySpark and Big Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rQRyufXhP6SL",
   "metadata": {
    "id": "rQRyufXhP6SL"
   },
   "source": [
    "(DS702, Zhiqiang Xu, MBZUAI) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bcaf1",
   "metadata": {
    "id": "ef2bcaf1"
   },
   "source": [
    "It’s becoming more common to face situations where the amount of data is simply too big to handle on a single machine. Luckily, technologies such as Apache Spark, Hadoop, and others have been developed to solve this exact problem. The power of those systems can be tapped into directly from Python using PySpark!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897e59c",
   "metadata": {
    "id": "6897e59c"
   },
   "source": [
    "In this lab, you'll learn: \n",
    "    <ul>\n",
    "    <li> What Python concepts can be applied to Big Data </li>\n",
    "    <li> How to use Apache Spark and PySpark </li>\n",
    "    <li> How to write basic PySpark programs </li>\n",
    "    <li> How to run PySpark programs on small datasets locally </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcecc8",
   "metadata": {
    "id": "babcecc8"
   },
   "source": [
    "## Big Data Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2bece",
   "metadata": {
    "id": "82c2bece"
   },
   "source": [
    "Despite its popularity as just a scripting language, Python exposes several programming paradigms like array-oriented programming, object-oriented programming, asynchronous programming, and many others. One paradigm that is of particular interest for aspiring Big Data professionals is [functional programming](https://en.wikipedia.org/wiki/Functional_programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324bc3b",
   "metadata": {
    "id": "1324bc3b"
   },
   "source": [
    "Functional programming is a common paradigm when you are dealing with Big Data. Writing in a functional manner makes for embarrassingly parallel code. This means it’s easier to take your code and have it run on several CPUs or even entirely different machines. You can work around the physical memory and CPU restrictions of a single workstation by running on multiple systems at once.\n",
    "\n",
    "This is the power of the PySpark ecosystem, allowing you to take functional code and automatically distribute it across an entire cluster of computers.\n",
    "\n",
    "The core idea of functional programming is that data should be manipulated by functions without maintaining any external state. This means that your code avoids global variables and always returns new data instead of manipulating the data in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13fd6c3",
   "metadata": {
    "id": "e13fd6c3"
   },
   "source": [
    "### Lambda Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb7721",
   "metadata": {
    "id": "f4fb7721"
   },
   "source": [
    "[Lambda functions](https://en.wikipedia.org/wiki/Anonymous_function) in Python are defined inline and are limited to a single expression. You’ve likely seen lambda functions when using the built-in `sorted()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d301b5f0",
   "metadata": {
    "id": "d301b5f0",
    "outputId": "c0a9397e-f363-4513-b1c5-3d7ff386f915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Big', 'awesome!', 'data', 'is', 'processing']\n"
     ]
    }
   ],
   "source": [
    "x = ['Big', 'data', 'processing', 'is', 'awesome!']\n",
    "print(sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926d4e3b",
   "metadata": {
    "id": "926d4e3b",
    "outputId": "a9358843-a956-4d30-c428-4fbdffa745ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome!', 'Big', 'data', 'is', 'processing']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(x, key = lambda arg: arg.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf805c9",
   "metadata": {
    "id": "caf805c9"
   },
   "source": [
    "The key parameter to sorted is called for each item in the iterable. This makes the sorting case-insensitive by changing all the strings to lowercase before the sorting takes place.\n",
    "\n",
    "This is a common use-case for lambda functions, small anonymous functions that maintain no external state.\n",
    "\n",
    "Other common functional programming functions exist in Python as well, such as `filter()`, `map()`, and `reduce()`. All these functions can make use of lambda functions or standard functions defined with `def` in a similar manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473c09b",
   "metadata": {
    "id": "e473c09b"
   },
   "source": [
    "###  `sorted()`,  `filter()`, `map()`, and `reduce()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621dd433",
   "metadata": {
    "id": "621dd433"
   },
   "source": [
    "The built-in `filter()`, `map()`, and `reduce()` functions are all common in functional programming. You’ll soon see that these concepts can make up a significant portion of the functionality of a PySpark program.\n",
    "\n",
    "It’s important to understand these functions in a core Python context. Then, you’ll be able to translate that knowledge into PySpark programs and the Spark API.\n",
    "\n",
    "`filter()` filters items out of an iterable based on a condition, typically expressed as a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821183c1",
   "metadata": {
    "id": "821183c1",
    "outputId": "ebd6d157-db54-4e17-f2a2-3ef55f609af8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big', 'data', 'is']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['Big', 'data', 'processing', 'is', 'awesome!']\n",
    "list(filter(lambda arg: len(arg) < 8, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35188aa",
   "metadata": {
    "id": "c35188aa"
   },
   "source": [
    "`filter()` takes an iterable, calls the `lambda` function on each item, and returns the items where the `lambda` returned True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2783f",
   "metadata": {
    "id": "e8d2783f"
   },
   "source": [
    "You can imagine using `filter()` to replace a common `for` loop pattern like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146e006b",
   "metadata": {
    "id": "146e006b",
    "outputId": "be032647-0339-4ebd-ab90-e0e2eb607156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Big', 'data', 'is']\n"
     ]
    }
   ],
   "source": [
    "def is_less_than_8_characters(item):\n",
    "    return len(item) < 8\n",
    "\n",
    "x = ['Big', 'data', 'processing', 'is', 'awesome!']\n",
    "results = []\n",
    "\n",
    "for item in x:\n",
    "    if is_less_than_8_characters(item):\n",
    "        results.append(item)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf8869",
   "metadata": {
    "id": "dbcf8869"
   },
   "source": [
    "This code collects all the strings that have less than 8 characters. The code is more verbose than the `filter()` example, but it performs the same function with the same results.\n",
    "\n",
    "Another less obvious benefit of `filter()` is that it returns an iterable. This means `filter()` doesn’t require that your computer have enough memory to hold all the items in the iterable at once. This is increasingly important with Big Data sets that can quickly grow to several gigabytes in size.\n",
    "\n",
    "`map()` is similar to `filter()` in that it applies a function to each item in an iterable, but it always produces a 1-to-1 mapping of the original items. The **new** iterable that `map()` returns will always have the same number of elements as the original iterable, which was not the case with `filter()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8b468e",
   "metadata": {
    "id": "cb8b468e",
    "outputId": "90c13b8e-0648-4407-a9ae-e8c077cc259a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BIG', 'DATA', 'PROCESSING', 'IS', 'AWESOME!']\n"
     ]
    }
   ],
   "source": [
    "x = ['Big', 'data', 'processing', 'is', 'awesome!']\n",
    "print(list(map(lambda arg: arg.upper(), x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eadb7df",
   "metadata": {
    "id": "2eadb7df"
   },
   "source": [
    "`map()` automatically calls the `lambda` function on all the items, effectively replacing a `for` loop like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7a1ff2",
   "metadata": {
    "id": "6d7a1ff2",
    "outputId": "ee3dde02-923c-408e-a600-65843234e868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BIG', 'DATA', 'PROCESSING', 'IS', 'AWESOME!']\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "x = ['Big', 'data', 'processing', 'is', 'awesome!']\n",
    "for item in x:\n",
    "    results.append(item.upper())\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833ec75",
   "metadata": {
    "id": "4833ec75"
   },
   "source": [
    "The for loop has the same result as the `map()` example, which collects all items in their upper-case form. However, as with the `filter()` example, `map()` returns an iterable, which again makes it possible to process large sets of data that are too big to fit entirely in memory.\n",
    "\n",
    "Finally, the last of the functional trio in the Python standard library is `reduce()`. As with `filter()` and `map()`, `reduce()` applies a function to elements in an iterable.\n",
    "\n",
    "Again, the function being applied can be a standard Python function created with the `def` keyword or a `lambda` function.\n",
    "\n",
    "However, `reduce()` doesn’t return a new iterable. Instead, `reduce()` uses the function called to reduce the iterable to a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9825c257",
   "metadata": {
    "id": "9825c257",
    "outputId": "8c45d276-4cf9-4786-fa3f-1b07360c185f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bigdataprocessingisawesome!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "x = ['Big', 'data', 'processing', 'is', 'awesome!']\n",
    "reduce(lambda val1, val2: val1 + val2, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5d069",
   "metadata": {
    "id": "75f5d069"
   },
   "source": [
    "This code combines all the items in the iterable, from left to right, into a single item. There is no call to `list()` here because `reduce()` already returns a single item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a8243",
   "metadata": {
    "id": "d82a8243"
   },
   "source": [
    "## Hello World in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067fbfe",
   "metadata": {
    "id": "b067fbfe"
   },
   "source": [
    "First, let's download `pyspark` library in your virtual environment if you're working in a jupyter notebook or [Google Colab](https://colab.research.google.com/) (preferred). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534fe11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "459d31fd",
   "metadata": {
    "id": "459d31fd",
    "outputId": "733fd2e2-596d-498b-9f4f-322fc617cddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Using legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: py4j, pyspark\n",
      "    Running setup.py install for pyspark: started\n",
      "    Running setup.py install for pyspark: finished with status 'done'\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f9406",
   "metadata": {
    "id": "bc9f9406"
   },
   "source": [
    "Now, let's download this text file and name it `copyright.txt` to work further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab51f4f",
   "metadata": {
    "id": "3ab51f4f",
    "outputId": "cf7c9695-eaa5-47b5-bb70-facad9cff8b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget -O copyright.txt 'https://raw.githubusercontent.com/ialbert/booleannet/master/COPYRIGHT.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e2bfb",
   "metadata": {
    "id": "051e2bfb"
   },
   "source": [
    "As in any programming language/framework, you’ll want to get started with a `Hello World` example. Below is the PySpark equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f6044",
   "metadata": {
    "id": "0c1f6044"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a2c21",
   "metadata": {
    "id": "dc2a2c21",
    "outputId": "b64d6f73-d66c-42e6-db2a-5ee5c18ce967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "txt = sc.textFile('copyright.txt')\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8511a",
   "metadata": {
    "id": "9cb8511a"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e18a7f",
   "metadata": {
    "id": "76e18a7f"
   },
   "source": [
    "You’ll learn all the details of this program soon, but take a good look. The program counts the total number of lines and the number of lines that have the word `python` in a file named `copyright.txt`.\n",
    "\n",
    "There can be a lot of things happening behind the scenes that distribute the processing across multiple nodes if you’re on a cluster. However, for now, think of the program as a Python program that uses the PySpark library.\n",
    "\n",
    "Now that you’ve seen some common functional concepts that exist in Python as well as a simple PySpark program, it’s time to dive deeper into Spark and PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d64b9",
   "metadata": {
    "id": "273d64b9"
   },
   "source": [
    "### What Is Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8038004",
   "metadata": {
    "id": "c8038004"
   },
   "source": [
    "Apache Spark is made up of several components, so describing it can be difficult. At its core, Spark is a generic engine for processing large amounts of data.\n",
    "\n",
    "Spark is written in Scala and runs on the JVM. Spark has built-in components for processing streaming data, machine learning, graph processing, and even interacting with data via SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9570ab",
   "metadata": {
    "id": "ba9570ab"
   },
   "source": [
    "### What Is PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2811df",
   "metadata": {
    "id": "5f2811df"
   },
   "source": [
    "Spark is implemented in Scala, a language that runs on the JVM, so how can you access all that functionality via Python?\n",
    "\n",
    "PySpark is the answer.\n",
    "\n",
    "The current version of PySpark is 3.2.0 and requires Python >= 3.6. \n",
    "\n",
    "PySpark is a Python-based wrapper on top of the Scala API. PySpark communicates with the Spark Scala-based API via the [Py4J library](https://www.py4j.org/). Py4J isn’t specific to PySpark or Spark. Py4J allows any Python program to talk to JVM-based code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436c310",
   "metadata": {
    "id": "2436c310"
   },
   "source": [
    "## PySpark API and Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bdb0c",
   "metadata": {
    "id": "6b4bdb0c"
   },
   "source": [
    "### RDD Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b1209",
   "metadata": {
    "id": "bd4b1209"
   },
   "source": [
    "To interact with PySpark, you create specialized data structures called **Resilient Distributed Datasets** (RDDs). RDDs hide all the complexity of transforming and distributing your data automatically across multiple nodes by a scheduler if you’re running on a cluster. \n",
    "\n",
    "In order to create an RDD, first, you need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a `builder()` or `newSession()` methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a `sc` variable of `SparkContext`. You can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using `stop()`) before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07c1c0",
   "metadata": {
    "id": "ba07c1c0"
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.master(\"local[1]\").appName(\"SparkExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f187ca6",
   "metadata": {
    "id": "7f187ca6"
   },
   "source": [
    "#### using `parellelize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2b894",
   "metadata": {
    "id": "c1a2b894"
   },
   "source": [
    "SparkContext has several functions to use with RDDs. For example, it’s `parallelize()` method is used to create an RDD from a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893140b4",
   "metadata": {
    "id": "893140b4",
    "outputId": "8d95e749-2c33-4b5a-a92f-97ce898156d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 100000), ('Java', 20000)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from parallelize    \n",
    "lst = [(\"Python\", 100000), (\"Java\", 20000), (\"Scala\", 3000)]\n",
    "rdd = spark.sparkContext.parallelize(lst)\n",
    "rdd.take(2) # takes first two elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07050c29",
   "metadata": {
    "id": "07050c29"
   },
   "source": [
    "#### using `textFile()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea28c1",
   "metadata": {
    "id": "adea28c1"
   },
   "source": [
    "RDD can also be created from a text file using `textFile()` function of the SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ca0ca",
   "metadata": {
    "id": "e87ca0ca",
    "outputId": "15748249-614a-4d2b-f95b-3bc0b83b21b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Note there are two third party packages included with BooleanNet, these are goverened by ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from external data source\n",
    "rdd2 = spark.sparkContext.textFile(\"copyright.txt\")\n",
    "rdd2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e9dc4",
   "metadata": {
    "id": "0d8e9dc4"
   },
   "source": [
    "Once you have an RDD, you can perform transformation and action operations. Any operation you perform on RDD runs in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4c10e",
   "metadata": {
    "id": "daf4c10e"
   },
   "source": [
    "### RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c8b83",
   "metadata": {
    "id": "428c8b83"
   },
   "source": [
    "On PySpark RDD, you can perform two kinds of operations.\n",
    "\n",
    "**RDD transformations** – Transformations are lazy operations. When you run a transformation(for example update), instead of updating a current RDD, these operations return another RDD. \n",
    "\n",
    "Transformations on Spark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are `flatMap()`, `map()`, `reduceByKey()`, `filter()`, `sortByKey()` and return new RDD instead of updating the current.\n",
    "\n",
    "**RDD actions** – operations that trigger computation and return RDD values to the driver.\n",
    "\n",
    "RDD Action operation returns the values from an RDD to a driver node. In other words, any RDD function that returns non RDD[T] is considered as an action. \n",
    "Some actions on RDD’s are `count()`, `collect()`, `first()`, `max()`, `reduce()` and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510aa3a",
   "metadata": {
    "id": "d510aa3a"
   },
   "source": [
    "### PySpark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abc675",
   "metadata": {
    "id": "a7abc675"
   },
   "source": [
    "DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "If you are coming from a Python background you already know what Pandas DataFrame is; PySpark DataFrame is mostly similar to Pandas DataFrame with exception PySpark DataFrames are distributed in the cluster (meaning the data in DataFrame’s are stored in different machines in a cluster) and any operations in PySpark executes in parallel on all machines whereas Panda Dataframe stores and operates on a single machine.\n",
    "\n",
    "If you have no Python background, we would recommend you learn some basics on Python (tutorials). For now, just know that data in PySpark DataFrame’s are stored in different machines in a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026a557",
   "metadata": {
    "id": "2026a557"
   },
   "source": [
    "### DataFrame Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73b408",
   "metadata": {
    "id": "de73b408"
   },
   "source": [
    "Simplest way to create DataFrame is from a Python list of data. DataFrame can also be created from RDD and by reading a files from several sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84febfbb",
   "metadata": {
    "id": "84febfbb"
   },
   "source": [
    "#### using `createDataFrame()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b89ca",
   "metadata": {
    "id": "064b89ca"
   },
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9462da",
   "metadata": {
    "id": "fe9462da"
   },
   "source": [
    "Since DataFrame’s are structure format which contains names and column, we can get the schema of the DataFrame using `df.printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b47dbf",
   "metadata": {
    "id": "b9b47dbf",
    "outputId": "2726e688-7090-490e-fe9d-8cc945e3e2c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dffeeff",
   "metadata": {
    "id": "2dffeeff"
   },
   "source": [
    "`df.show()` shows the 20 elements from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a98ac",
   "metadata": {
    "id": "f62a98ac",
    "outputId": "3250c754-9fe1-4a5b-9aaa-d3c331bd9500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733108a",
   "metadata": {
    "id": "5733108a"
   },
   "source": [
    "#### DataFrame from external data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc1c4f",
   "metadata": {
    "id": "dffc1c4f"
   },
   "source": [
    "In realtime applications, DataFrame’s are created from external sources like files from the local system, HDFS, S3 Azure, HBase, MySQL table e.t.c. Below is an example of how to read a csv file from a local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f721634",
   "metadata": {
    "id": "0f721634",
    "outputId": "b966c1a3-d23f-4165-f062-7f60668b3109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-01 14:59:34--  https://raw.githubusercontent.com/btucker/thisweknow/master/script/zipcodes.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 784128 (766K) [text/plain]\n",
      "Saving to: ‘zipcodes.csv’\n",
      "\n",
      "zipcodes.csv        100%[===================>] 765.75K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2022-01-01 14:59:35 (26.0 MB/s) - ‘zipcodes.csv’ saved [784128/784128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O zipcodes.csv 'https://raw.githubusercontent.com/btucker/thisweknow/master/script/zipcodes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bd0d9",
   "metadata": {
    "id": "221bd0d9",
    "outputId": "bc887120-2b80-4a07-ca49-fba5579371bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ZIP Code: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State Abbreviation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"zipcodes.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01335599",
   "metadata": {
    "id": "01335599",
    "outputId": "31fdf25f-c672-4dce-8d1a-524d9b3f28c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+\n",
      "|ZIP Code|      City|State Abbreviation|\n",
      "+--------+----------+------------------+\n",
      "|     210|PORTSMOUTH|                NH|\n",
      "|     211|PORTSMOUTH|                NH|\n",
      "|     212|PORTSMOUTH|                NH|\n",
      "|     213|PORTSMOUTH|                NH|\n",
      "|     214|PORTSMOUTH|                NH|\n",
      "|     215|PORTSMOUTH|                NH|\n",
      "|     501|HOLTSVILLE|                NY|\n",
      "|     544|HOLTSVILLE|                NY|\n",
      "|     601|  ADJUNTAS|                PR|\n",
      "|     602|    AGUADA|                PR|\n",
      "|     603| AGUADILLA|                PR|\n",
      "|     604| AGUADILLA|                PR|\n",
      "|     605| AGUADILLA|                PR|\n",
      "|     606|   MARICAO|                PR|\n",
      "|     610|    ANASCO|                PR|\n",
      "|     611|   ANGELES|                PR|\n",
      "|     612|   ARECIBO|                PR|\n",
      "|     613|   ARECIBO|                PR|\n",
      "|     614|   ARECIBO|                PR|\n",
      "|     616|  BAJADERO|                PR|\n",
      "+--------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e711d1c",
   "metadata": {
    "id": "5e711d1c"
   },
   "source": [
    "### PySpark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94049b25",
   "metadata": {
    "id": "94049b25"
   },
   "source": [
    "In order to use SQL, first, create a temporary table on DataFrame using `createOrReplaceTempView()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1411ca",
   "metadata": {
    "id": "2a1411ca"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"ZIPCODES_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6571a",
   "metadata": {
    "id": "9df6571a"
   },
   "source": [
    "Once created, this table can be accessed throughout the SparkSession using `sql()` and it will be dropped along with your `SparkContext` termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ee134",
   "metadata": {
    "id": "864ee134",
    "outputId": "29774409-67cc-4732-b2ac-67ec2bd13ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+\n",
      "|ZIP Code|      City|State Abbreviation|\n",
      "+--------+----------+------------------+\n",
      "|     210|PORTSMOUTH|                NH|\n",
      "|     211|PORTSMOUTH|                NH|\n",
      "|     212|PORTSMOUTH|                NH|\n",
      "|     213|PORTSMOUTH|                NH|\n",
      "|     214|PORTSMOUTH|                NH|\n",
      "|     215|PORTSMOUTH|                NH|\n",
      "|     501|HOLTSVILLE|                NY|\n",
      "|     544|HOLTSVILLE|                NY|\n",
      "|     601|  ADJUNTAS|                PR|\n",
      "|     602|    AGUADA|                PR|\n",
      "|     603| AGUADILLA|                PR|\n",
      "|     604| AGUADILLA|                PR|\n",
      "|     605| AGUADILLA|                PR|\n",
      "|     606|   MARICAO|                PR|\n",
      "|     610|    ANASCO|                PR|\n",
      "|     611|   ANGELES|                PR|\n",
      "|     612|   ARECIBO|                PR|\n",
      "|     613|   ARECIBO|                PR|\n",
      "|     614|   ARECIBO|                PR|\n",
      "|     616|  BAJADERO|                PR|\n",
      "+--------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from ZIPCODES_DB\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eac11",
   "metadata": {
    "id": "029eac11"
   },
   "source": [
    "Next, we can see schema of `ZIPCODES_DB` database by creating new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed799b",
   "metadata": {
    "id": "0aed799b",
    "outputId": "fd49f4e8-ecf9-4743-f6d9-d94c74c846b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ZIP Code: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State Abbreviation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"SELECT * from ZIPCODES_DB\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6353c9",
   "metadata": {
    "id": "8b6353c9"
   },
   "source": [
    "Here, we calculated the number of rows related to the `City` column and showed the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac96e7",
   "metadata": {
    "id": "3aac96e7",
    "outputId": "d9f57d3b-90f6-484c-e736-93ea3f3f08a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      City|ENTRIES|\n",
      "+----------+-------+\n",
      "|WASHINGTON|    301|\n",
      "|   HOUSTON|    190|\n",
      "|  NEW YORK|    162|\n",
      "|   EL PASO|    158|\n",
      "|    DALLAS|    130|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT City, COUNT(*) AS ENTRIES from ZIPCODES_DB GROUP BY City ORDER BY ENTRIES DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ce31a",
   "metadata": {
    "id": "ea6ce31a"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e804c2",
   "metadata": {
    "id": "50e804c2"
   },
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f8a4a",
   "metadata": {
    "id": "7c1f8a4a",
    "outputId": "948dc897-19f5-4114-a319-7c6342a27850"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('note', 1),\n",
       " ('there', 1),\n",
       " ('are', 2),\n",
       " ('two', 1),\n",
       " ('third', 1),\n",
       " ('party', 1),\n",
       " ('packages', 1),\n",
       " ('included', 2),\n",
       " ('with', 3),\n",
       " ('booleannet,', 1),\n",
       " ('these', 1),\n",
       " ('goverened', 1),\n",
       " ('by', 1),\n",
       " ('the', 17),\n",
       " ('licenses', 1),\n",
       " ('below', 1),\n",
       " ('and', 5),\n",
       " ('have', 1),\n",
       " ('different', 1),\n",
       " ('copyrights.', 1),\n",
       " ('ply:', 1),\n",
       " ('gnu', 1),\n",
       " ('lesser', 1),\n",
       " ('general', 1),\n",
       " ('public', 1),\n",
       " ('license,', 2),\n",
       " ('copyright', 5),\n",
       " ('(c)', 3),\n",
       " ('2001-2007,', 1),\n",
       " ('david', 1),\n",
       " ('m.', 1),\n",
       " ('beazley', 1),\n",
       " ('functional:', 1),\n",
       " ('python', 2),\n",
       " ('software', 8),\n",
       " ('foundation', 2),\n",
       " ('2004-2006', 1),\n",
       " ('rest', 1),\n",
       " ('of', 9),\n",
       " ('is', 4),\n",
       " ('licensed', 1),\n",
       " ('mit', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('license', 1),\n",
       " ('2007,', 1),\n",
       " ('istvan', 1),\n",
       " ('albert', 1),\n",
       " ('permission', 2),\n",
       " ('hereby', 1),\n",
       " ('granted,', 1),\n",
       " ('free', 1),\n",
       " ('charge,', 1),\n",
       " ('to', 8),\n",
       " ('any', 3),\n",
       " ('person', 1),\n",
       " ('obtaining', 1),\n",
       " ('a', 2),\n",
       " ('copy', 1),\n",
       " ('this', 2),\n",
       " ('associated', 1),\n",
       " ('documentation', 1),\n",
       " ('files', 1),\n",
       " ('(the', 1),\n",
       " ('\"software\"),', 1),\n",
       " ('deal', 1),\n",
       " ('in', 6),\n",
       " ('without', 3),\n",
       " ('restriction,', 1),\n",
       " ('including', 2),\n",
       " ('limitation', 1),\n",
       " ('rights', 1),\n",
       " ('use,', 1),\n",
       " ('copy,', 1),\n",
       " ('modify,', 1),\n",
       " ('merge,', 1),\n",
       " ('publish,', 1),\n",
       " ('distribute,', 1),\n",
       " ('sublicense,', 1),\n",
       " ('and/or', 1),\n",
       " ('sell', 1),\n",
       " ('copies', 2),\n",
       " ('software,', 1),\n",
       " ('permit', 1),\n",
       " ('persons', 1),\n",
       " ('whom', 1),\n",
       " ('furnished', 1),\n",
       " ('do', 1),\n",
       " ('so,', 1),\n",
       " ('subject', 1),\n",
       " ('following', 1),\n",
       " ('conditions:', 1),\n",
       " ('above', 1),\n",
       " ('notice', 2),\n",
       " ('shall', 2),\n",
       " ('be', 2),\n",
       " ('all', 1),\n",
       " ('or', 8),\n",
       " ('substantial', 1),\n",
       " ('portions', 1),\n",
       " ('software.', 2),\n",
       " ('provided', 1),\n",
       " ('\"as', 1),\n",
       " ('is\",', 1),\n",
       " ('warranty', 1),\n",
       " ('kind,', 1),\n",
       " ('express', 1),\n",
       " ('implied,', 1),\n",
       " ('but', 1),\n",
       " ('not', 1),\n",
       " ('limited', 1),\n",
       " ('warranties', 1),\n",
       " ('merchantability,', 1),\n",
       " ('fitness', 1),\n",
       " ('for', 2),\n",
       " ('particular', 1),\n",
       " ('purpose', 1),\n",
       " ('noninfringement.', 1),\n",
       " ('no', 1),\n",
       " ('event', 1),\n",
       " ('authors', 1),\n",
       " ('holders', 1),\n",
       " ('liable', 1),\n",
       " ('claim,', 1),\n",
       " ('damages', 1),\n",
       " ('other', 2),\n",
       " ('liability,', 1),\n",
       " ('whether', 1),\n",
       " ('an', 1),\n",
       " ('action', 1),\n",
       " ('contract,', 1),\n",
       " ('tort', 1),\n",
       " ('otherwise,', 1),\n",
       " ('arising', 1),\n",
       " ('from,', 1),\n",
       " ('out', 1),\n",
       " ('connection', 1),\n",
       " ('use', 1),\n",
       " ('dealings', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext(\"local\", \"Word Count\")\n",
    "txt = sc.textFile('copyright.txt') # list of line of sentences\n",
    "words = txt.flatMap(lambda line: line.split()) # transforming sentences into separate words\n",
    "wordCounts = words.map(lambda word: (word.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd73c91",
   "metadata": {
    "id": "cfd73c91"
   },
   "source": [
    "Next, briefly about steps: \n",
    "We created a SparkContext to connect connect the Driver that runs locally.\n",
    "\n",
    "`sc = SparkContext(\"local\", \"Word Count\")`\n",
    "\n",
    "Next, we read the input text file using SparkContext variable and created a flatmap of words. `words` is of type PythonRDD.\n",
    "\n",
    "`txt = sc.textFile('copyright.txt')`\n",
    "\n",
    "`words = txt.flatMap(lambda line: line.split())` \n",
    "we have split the words using single space as separator.\n",
    "\n",
    "Then we will map each word to a key:value pair of word:1, 1 being the number of occurrences.\n",
    "\n",
    "`words.map(lambda word: (word.lower(), 1))` \n",
    "\n",
    "The result is then reduced by key, which is the word, and the values are added.\n",
    "\n",
    "`reduceByKey(lambda a, b: a + b)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac92b05",
   "metadata": {
    "id": "5ac92b05"
   },
   "outputs": [],
   "source": [
    "# to stop SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc6ebc",
   "metadata": {
    "id": "54fc6ebc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab1. PySpark Setup and Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
