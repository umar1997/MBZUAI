{"cells":[{"cell_type":"markdown","metadata":{"id":"d8v9DsuPw3Lb"},"source":["# Information Extraction using Spacy\n","\n","With the huge amount of text data being generated every hour, extracting useful information from these documents becomes a daunting task. Information extraction in natural language processing aims to develop algorithms that can automate the process of extracting key information from large numbers of documents.\n","\n","Information extraction remains a non-trivial task and is an active area of research as it requires a deep understanding of the meaning of the text. Example use cases of event extraction are discovering and extracting details about crises and incidents from social media (such as location) and extracting medication and associated side effects from medical reports.\n","\n","In this notebook, we will demonstrate how to build a simple information extraction framework based on rule-based association and dependency parsing. This method is based on an approach used to extract information about armed conflicts [here](https://andrewhalterman.com/post/event-data-in-30-lines-of-python/).\n","\n","Dependency parsing is the task of identifying the grammatical relationships between the words in a text. For example, in the sentence \"Jane is making a salad.\", \"Jane\" is the nominal subject of the verb \"making\" and \"salad\" is its direct object. By traversing these grammatical dependencies, we can trace the entities involved in a certain action. Therefore, if we can detect a trigger action/event, we can extract involved parties.\n","\n","In this notebook, we will demonstrate this on the [JUSTICE](https://arxiv.org/abs/2112.03414) dataset which provides a collection of documents detailing the results of court proceedings. We will try to extract information regarding the legal action taken (conviction, appeal...) and the entities involved or related to the action.\n","\n","For dependency parsing, we will rely on the parser in the spaCy processing pipeline. We begin by downloading the pretrained spaCy model \"en_core_web_lg\" and loading it into a processing pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lq6JyiJQzt46"},"outputs":[],"source":["%%capture\n","#!pip uninstall -q neuralcoref -y\n","#!pip install -q neuralcoref --no-binary neuralcoref\n","\n","#!pip uninstall -q spacy -y\n","#!pip install -q -U spacy==2.1.0\n","\n","import spacy.cli\n","spacy.cli.download('en_core_web_lg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9zr7TTn9_60"},"outputs":[],"source":["import spacy\n","\n","import en_core_web_lg\n","nlp = en_core_web_lg.load()\n","\n","#import neuralcoref\n","#neuralcoref.add_to_pipe(nlp)\n","#print(nlp.pipe_names)"]},{"cell_type":"markdown","source":["We can now try spaCy's parser on the example sentenced we used above."],"metadata":{"id":"VCmuNynbpoNC"}},{"cell_type":"code","source":["doc = nlp('Jane is making a Greek salad.')\n","\n","for token in doc:\n","    print(f'{token.text}, {token.dep_}')"],"metadata":{"id":"vVPnARZiqz54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When calling the processing pipeline on a text, it will return a spaCy Doc object which contains the tokenized text along with other NLP attributes that were computed by the pipeline. Since we are interested in grammatical dependencies, we iterate over the tokens and print the dependency property for each word. We see some familiar grammatical tags such as nominal subject (nsubj), auxiliary (aux) and determinant (det).\n","\n","However, this does not show the actual relationships between the tokens but only the grammatical functions. For instance, we see that \"is\" is an auxiliary but it is not explicitly stated that it is the auxiliary of \"making\". Luckily, spaCy has a useful visualization tool that allows us to see these dependencies."],"metadata":{"id":"CX1J6vH-rHos"}},{"cell_type":"code","source":["from spacy import displacy\n","\n","displacy.render(doc, style='dep', jupyter=True)"],"metadata":{"id":"HJ0HDv92sWLU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In case a certain dependency is not familiar, we can query a clarification from spaCy using the following function."],"metadata":{"id":"FBUPyXzasudr"}},{"cell_type":"code","source":["spacy.explain('amod')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414},"id":"1BOoj5e5s33S","executionInfo":{"status":"ok","timestamp":1645948294263,"user_tz":-120,"elapsed":20,"user":{"displayName":"Mariette Awad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBUe7YIsZ4XYsb9nmLBVo0I8r7gs4qRrDxOGOgcg=s64","userId":"02461350705414261288"}},"outputId":"8248f60e-f8a1-46b5-9445-1e49bf72cf28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'adjectival modifier'"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Another interesting property that can be visualized in the named entity tag for each token. We can similarly explain unclear entity tags."],"metadata":{"id":"4D4h7dAks7cx"}},{"cell_type":"code","source":["displacy.render(doc, style='ent', jupyter=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"Gkvxe4VttEov","executionInfo":{"status":"ok","timestamp":1645948294264,"user_tz":-120,"elapsed":19,"user":{"displayName":"Mariette Awad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBUe7YIsZ4XYsb9nmLBVo0I8r7gs4qRrDxOGOgcg=s64","userId":"02461350705414261288"}},"outputId":"52a950e0-9c95-4b3a-aa5e-b28ebb2c3616"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Jane\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n","</mark>\n"," is making a \n","<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Greek\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n","</mark>\n"," salad.</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","source":["spacy.explain('NORP')"],"metadata":{"id":"QXktzRtVtMch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Integrating NeuralCoref"],"metadata":{"id":"AY0lE9RXPbJi"}},{"cell_type":"code","source":["'''\n","def get_coref(doc):\n","    if doc._.has_coref:\n","        return nlp(doc._.coref_resolved)\n","    else:\n","        return doc\n","\n","from spacy.tokens import Doc\n","\n","Doc.set_extension('coref', getter=get_coref)\n","'''"],"metadata":{"id":"vECs9s1ePaQu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8sPPH8JyCKv"},"source":["## Dataset\n","\n","Before we can understand how these dependencies can help us in our task, let's first load the dataset. We are only interested in the documents under the \"facts\" column. Also, we remove the \\<p>\\</p> tags indicating the beginning and end of a paragraph."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPXoUJblyE6j"},"outputs":[],"source":["import pandas as pd\n","\n","texts = pd.read_csv('https://raw.githubusercontent.com/smitp415/CSCI_544_Final_Project/main/clean_data.csv')\n","texts['facts'] = texts['facts'].apply(lambda x: x.replace('<p>', '').replace('</p>','').replace('\\n',''))\n","texts = texts['facts'].values\n","print('\\n**************\\n'.join(texts[:5]))"]},{"cell_type":"markdown","source":["Let us examine the dependencies of an example from the dataset (we only select the first sentence of the paragraph to reduce the visualization and only focus on what we're highlighting)."],"metadata":{"id":"-Tl5W_oqwgLE"}},{"cell_type":"code","source":["displacy.render(nlp(texts[9])[:13], style='dep', jupyter=True)"],"metadata":{"id":"aVmRkGx5wi7p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the context of our application, this text tells that a certain Johnny Wilson got convicted by the court. If we follow the reasoning described in our approach, \"convicted\" would be the trigger token signaling the event of interest. If we follow the dependecies related to this token we can find \"Wilson\" as being the object and \"court\" as the subject. This is important information because it tells us who was involved in this action and who was affected.\n","\n","Let us see how we can write a code to extract this information."],"metadata":{"id":"8j7Ter7CXv2G"}},{"cell_type":"code","source":["for token in nlp(texts[9])[:13]:\n","    #first we need to find our trigger word\n","    if token.text == 'convicted':\n","        #iterate over all tokens related by dependency (children)\n","        for child in token.children:\n","            if child.dep_ == 'nsubj':\n","                subj = child\n","            elif child.dep_ == 'dobj':\n","                obj = child\n","        print(f'Subj: {subj.text}, Trigger: {token.text}, Obj: {obj.text}')"],"metadata":{"id":"58mYVwu3X6ae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As expected, the code above extracts the needed information. However, in this example, we knew exactly the trigger word we wanted. Luckily, all of our documents have a legal context. Therefore, we define a list of potential trigger words that are likely to appear in our dataset."],"metadata":{"id":"L5ruLOazZ2L0"}},{"cell_type":"code","source":["trigger_list = ['file', 'sue', 'appeal', 'reject', 'convict', 'refuse', 'deny',\n","             'rule', 'plead', 'affirm', 'refuse', 'charge', 'accuse', 'challenge',\n","             'sentence', 'prosecute']"],"metadata":{"id":"mFj7hq88bS-a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When matching tokens against this list, we will be matching the word lemmas instead of the actual words. Word lemmas are the root form of words. This will ensure that we do not miss any triggers due to a difference in word form. Below is an example of word lemmatization which is part of spaCy's default processing pipeline."],"metadata":{"id":"g3kc6DWYbgbU"}},{"cell_type":"code","source":["print(nlp('pleaded')[0].lemma_)"],"metadata":{"id":"2nIfpQOVbd_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we extend the code shown above to look for multiple events in a document. Instead of extracting each detected component as a single word, we extract the full token subtree (meaning every token connected by dependency) to add more context to the extracted information. We then visualize a number of samples from the dataset. We also account for more grammatical dependecies such as passive subjects occuring in passive clauses."],"metadata":{"id":"dpGL2tOpPdkO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vTPmxsw-wEb"},"outputs":[],"source":["def detect_event(doc):\n","    #match word lemmas against trigger words\n","    events = []\n","    for token in doc:\n","        if token.lemma_ in trigger_list:\n","            events.append(token)\n","    return events\n","                    \n","def actor_extractor(event):\n","    #follow the trigger's dependency to find obj and subj tokens\n","    nsubj = None\n","    dobj = None\n","    for child in event.children:\n","        if child.dep_ in ['nsubj', 'csubj']:\n","            nsubj = child\n","        elif child.dep_ in ['dobj', 'nsubjpass', 'csubjpass']:\n","            dobj = child\n","    return nsubj, dobj\n","\n","def extract_info(doc, use_coref=False, display_results=True):\n","    if use_coref:\n","        doc = doc._.coref\n","    if display_results:\n","        displacy.render(doc, style='ent', jupyter=True)\n","        print()\n","    events = detect_event(doc)\n","    #check if all three components of the event were detected\n","    full_event = False\n","    if events:\n","        for event in events:\n","            nsubj, dobj = actor_extractor(event)\n","            #check if all three components of the event were detected\n","            if nsubj and dobj:\n","                full_event=True\n","            if display_results:\n","                if nsubj:\n","                    print(f\"Subject: {''.join(w.text_with_ws for w in nsubj.subtree).strip()}\")\n","                else:\n","                    print('Subject: not found')\n","                print(f\"Event: {event.text}\")\n","                if dobj:\n","                    print(f\"Object: {''.join(w.text_with_ws for w in dobj.subtree).strip()}\")\n","                else:\n","                    print('Object: not found')\n","                print('***************************')\n","    else:\n","        if display_results:\n","            print('No event detected')\n","    if display_results:\n","        print('***************************\\n'*3)\n","    else:\n","        return full_event\n","\n","for doc in nlp.pipe(texts[5:10]):\n","    extract_info(doc)"]},{"cell_type":"markdown","source":["Inspecting the results shows that in many cases, our algorithm was able to extract meaningful information from the text.\n","\n","For instance, if we look at sample at index #5, we find two complete events telling us that the case is about someone filing for a second Visa application after the court had refused his first application. This is already delivering information in a compacted form from a relatively long paragraph.\n","\n","To get a more quantitative assesment of our approach, we added a flag \"full_event\" to the code above. This flag is raised whenever we detect at least one event with all three components for each document. We run the algorithm on a large number of samples. We tried multiple sample sizes (the nlp pipeline tended to become slow when running on significantly large sets) and found that we consistently scored above 60%."],"metadata":{"id":"ygAuO3UwWOlp"}},{"cell_type":"code","source":["count = 0\n","data = texts[:500]\n","for doc in nlp.pipe(data):\n","    full_event = extract_info(doc, use_coref=False, display_results=False)\n","    if full_event:\n","        count += 1\n","print(f'Detected {count}/{len(data)} ({count/len(data)*100}%) complete events.')"],"metadata":{"id":"cUv6fZpEUvMf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645948312264,"user_tz":-120,"elapsed":14961,"user":{"displayName":"Mariette Awad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBUe7YIsZ4XYsb9nmLBVo0I8r7gs4qRrDxOGOgcg=s64","userId":"02461350705414261288"}},"outputId":"2401225b-ad35-48c2-fad4-6c0cbf6b4666"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected 309/500 (61.8%) complete events.\n"]}]},{"cell_type":"markdown","source":["## Detection rate with Coref"],"metadata":{"id":"ZnkH-Js1WR9P"}},{"cell_type":"code","source":["'''\n","count = 0\n","data = texts[:100]\n","for doc in nlp.pipe(data):\n","    full_event = extract_info(doc, use_coref=True, display_results=False)\n","    if full_event:\n","        count += 1\n","print(f'Detected {count}/{len(data)} ({count/len(data)*100}%) complete events.')\n","'''"],"metadata":{"id":"C8Bo0GioWMEY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Weaknesses of this approach\n","\n","The most glaring weakness of our algorithms is that it is relies on some pattern matching and largely dependent on manually defined trigger vocabulary and grammatical dependency matching. Naturally, performance can be improved by making the trigger list more comprehensive but this is not something that can be efficiently scaled.\n","\n","Additionally, there are some cases where the dependency parser fails to detect some relationships that result in missing some event components. For instance, in the example below, \"charged\" is detected as an event but without \"John Argersinger\" as the person charged. This is due to the structure of the sentence where the first part \"Jon Argersinger was an indigent\" forms a complete meaningful phrase and \"was\" is not detected as the auxiliary to \"charged\" therefore ignoring any connection between \"John\" and \"charged\".\n","\n","In the same example, \"convicted\" is detected with the passive subject \"he\" but \"sentenced\" which follows by conjunction is not attributed the same subject."],"metadata":{"id":"2P4t2ge4eXpP"}},{"cell_type":"code","source":["extract_info(nlp(texts[7]))"],"metadata":{"id":"-OV45rAoiI2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["displacy.render(nlp(texts[7]), style='dep', jupyter=True)"],"metadata":{"id":"y6pQNAWfhtht"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"IE.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}