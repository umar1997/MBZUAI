{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Analysis. PageRank Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nurbek.tastan/.conda/envs/bigdata/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/17 08:12:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('pagerank').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. \n",
    "\n",
    "The basic structure on which the algorithm works best is a directed network. E.g, WWW can be represented as a huge directed network, where websites are the nodes and hyperlinks amongst the pages are the directed edges.\n",
    "\n",
    "#### Example: \n",
    "\n",
    "Suppose, there are 5 websites: A, B, C, D and E, where A hyperlinks to B; B hyperlinks to D, E; C hyperlinks to B; D hyperlinks to A, C, E and lastly E hyperlinks to A. The directed network figure is given below.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*WePw-05wGpkamKVI3cfqew.png)\n",
    "\n",
    "According to Google, the importance of a page means the number and quality of inward links of that page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to calculate PageRank centrality of each node\n",
    "\n",
    "**Step 1:** Assign each node with an initial value of 1/n, where n is the number of nodes.\n",
    "\n",
    "**Step 2:** For each node, `n(i)` in the network, find all the nodes, `r = [n(1),n(2),..]` being referred by n(i), and assign all those referred nodes as `r(i) = previous PageRank value of node n(i) / number of nodes being referred to by n(i)`, i.e, size of `r`\n",
    "\n",
    "**Step 3:** Repeat Step 2 until convergence \n",
    "\n",
    "We can initialize the nodes with any value. The algorithm will converge always, irrespective of what values the nodes are initialized with. This is due to the principal eigenvector of the PageRank matrix which doesnâ€™t depend on the initial values you assign each node with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's work with `links.txt` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A B', 'B A C', 'C B D', 'D C']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjacency list\n",
    "links = sc.textFile('links.txt')\n",
    "links.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `'A B'` means that A out-links to B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links: [('A', ['B']), ('B', ['A', 'C']), ('C', ['B', 'D']), ('D', ['C'])]\n",
      "# of nodes: 4\n",
      "ranks: [('A', 0.25), ('B', 0.25), ('C', 0.25), ('D', 0.25)]\n"
     ]
    }
   ],
   "source": [
    "# Key/value pairs\n",
    "links = links.map(lambda x: (x.split(' ')[0], x.split(' ')[1:]))\n",
    "print('links:', links.collect())\n",
    " \n",
    "# Find node count\n",
    "N = links.count()\n",
    "print('# of nodes:', N)\n",
    "\n",
    "# Create and initialize the ranks\n",
    "ranks = links.map(lambda node: (node[0], 1.0 / N))\n",
    "print('ranks:', ranks.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 0.125), ('B', 0.375), ('C', 0.375), ('D', 0.125)]\n",
      "[('A', 0.1875), ('B', 0.3125), ('C', 0.3125), ('D', 0.1875)]\n",
      "[('A', 0.15625), ('B', 0.34375), ('C', 0.34375), ('D', 0.15625)]\n",
      "[('A', 0.171875), ('B', 0.328125), ('C', 0.328125), ('D', 0.171875)]\n",
      "[('A', 0.1640625), ('B', 0.3359375), ('C', 0.3359375), ('D', 0.1640625)]\n",
      "[('A', 0.16796875), ('B', 0.33203125), ('C', 0.33203125), ('D', 0.16796875)]\n",
      "[('A', 0.166015625), ('B', 0.333984375), ('C', 0.333984375), ('D', 0.166015625)]\n",
      "[('A', 0.1669921875), ('B', 0.3330078125), ('C', 0.3330078125), ('D', 0.1669921875)]\n",
      "[('A', 0.16650390625), ('B', 0.33349609375), ('C', 0.33349609375), ('D', 0.16650390625)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 0.166748046875), ('B', 0.333251953125), ('C', 0.333251953125), ('D', 0.166748046875)]\n"
     ]
    }
   ],
   "source": [
    "iter = 10 # number of iterations\n",
    "for i in range(iter):\n",
    "    # (rank/(number of neighbors) \n",
    "    ranks = links.join(ranks).flatMap(lambda x : [(i, float(x[1][1])/len(x[1][0])) for i in x[1][0]])\\\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    print(ranks.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 0.333251953125),\n",
       " ('C', 0.333251953125),\n",
       " ('A', 0.166748046875),\n",
       " ('D', 0.166748046875)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list elements in descending order\n",
    "ranks.sortBy(lambda x: x[1], ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65b605b1ebac98c1535b24e77f9feff4a0d49b5155481ff8a814ffe3e13ad93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
