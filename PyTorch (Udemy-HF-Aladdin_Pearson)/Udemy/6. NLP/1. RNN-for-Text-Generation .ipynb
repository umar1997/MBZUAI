{"cells":[{"cell_type":"markdown","source":["Link: https://pytorch.org/docs/master/nn.html"],"metadata":{"id":"GqFRJFBedIny"}},{"cell_type":"markdown","metadata":{"id":"4-QKhpneBz0F"},"source":["\n","# RNN for Text Generation\n","\n","## Generating Text (encoded variables)\n","\n","We saw how to generate continuous values, now let's see how to generalize this to generate categorical sequences (such as words or letters).\n","\n","## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tFjydrqOBz0W","executionInfo":{"status":"ok","timestamp":1643618838756,"user_tz":-240,"elapsed":4924,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"4-yy69lfBz0Z"},"source":["## Get Text Data"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcRuKP-HFzjG","executionInfo":{"status":"ok","timestamp":1643618869476,"user_tz":-240,"elapsed":26723,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"86c05371-ea52-4c1b-8e27-ee287db30384"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at drive\n"]}]},{"cell_type":"code","source":["import os\n","import zipfile\n","import numpy as np\n","\n","dir = \"/content/drive/My Drive/Colab Notebooks/PyTorch/Udemy/Data/\"\n","files = os.listdir(dir)\n","files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_OgvbvGeF3LE","executionInfo":{"status":"ok","timestamp":1643618898831,"user_tz":-240,"elapsed":2139,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"61462183-358c-48f2-a724-e5b7fbe004c4"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['meditations_by_marcus_aurelius.txt',\n"," 'NYCTaxiFares.csv',\n"," 'war_and_peace.txt',\n"," 'sinewave.csv',\n"," 'iris.csv',\n"," 'TomSawyer.txt',\n"," 'UK_Food',\n"," 'income.csv',\n"," 'bank.csv',\n"," 'shakespeare.txt',\n"," 'pride_and_prejudice.txt',\n"," '.ipynb_checkpoints',\n"," 'TimeSeriesData',\n"," 'MNIST',\n"," 'FashionMNIST']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IJDGMEuBBz0a","executionInfo":{"status":"ok","timestamp":1643618902788,"user_tz":-240,"elapsed":1194,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["with open(dir + 'shakespeare.txt','r',encoding='utf8') as f:\n","    text = f.read()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"iucdQQ_GBz0b","executionInfo":{"status":"ok","timestamp":1643618904676,"user_tz":-240,"elapsed":16,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"582479e7-87fa-41b2-c0e2-712deb3f5157"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""]},"metadata":{},"execution_count":5}],"source":["text[:1000]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SAKfBmULBz0d","executionInfo":{"status":"ok","timestamp":1643618907582,"user_tz":-240,"elapsed":9,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"5c4309e0-6b5a-438a-876a-21de6d22c8f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","                     1\n","  From fairest creatures we desire increase,\n","  That thereby beauty's rose might never die,\n","  But as the riper should by time decease,\n","  His tender heir might bear his memory:\n","  But thou contracted to thine own bright eyes,\n","  Feed'st thy light's flame with self-substantial fuel,\n","  Making a famine where abundance lies,\n","  Thy self thy foe, to thy sweet self too cruel:\n","  Thou that art now the world's fresh ornament,\n","  And only herald to the gaudy spring,\n","  Within thine own bud buriest thy content,\n","  And tender churl mak'st waste in niggarding:\n","    Pity the world, or else this glutton be,\n","    To eat the world's due, by the grave and thee.\n","\n","\n","                     2\n","  When forty winters shall besiege thy brow,\n","  And dig deep trenches in thy beauty's field,\n","  Thy youth's proud livery so gazed on now,\n","  Will be a tattered weed of small worth held:  \n","  Then being asked, where all thy beauty lies,\n","  Where all the treasure of thy lusty days;\n","  To say within thine own deep su\n"]}],"source":["print(text[:1000])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvFsiHQTBz0e","executionInfo":{"status":"ok","timestamp":1643618913556,"user_tz":-240,"elapsed":923,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"0755511b-87c3-4fb8-eb86-3f9750c842fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5445609"]},"metadata":{},"execution_count":7}],"source":["len(text)"]},{"cell_type":"markdown","metadata":{"id":"3D3cJzA8Bz0g"},"source":["## Encode Entire Text"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uvbf0pEQBz0h","executionInfo":{"status":"ok","timestamp":1643618916358,"user_tz":-240,"elapsed":9,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"3ec1733a-151b-412c-b130-7894bc9199cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'\\n',\n"," ' ',\n"," '!',\n"," '\"',\n"," '&',\n"," \"'\",\n"," '(',\n"," ')',\n"," ',',\n"," '-',\n"," '.',\n"," '0',\n"," '1',\n"," '2',\n"," '3',\n"," '4',\n"," '5',\n"," '6',\n"," '7',\n"," '8',\n"," '9',\n"," ':',\n"," ';',\n"," '<',\n"," '>',\n"," '?',\n"," 'A',\n"," 'B',\n"," 'C',\n"," 'D',\n"," 'E',\n"," 'F',\n"," 'G',\n"," 'H',\n"," 'I',\n"," 'J',\n"," 'K',\n"," 'L',\n"," 'M',\n"," 'N',\n"," 'O',\n"," 'P',\n"," 'Q',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'U',\n"," 'V',\n"," 'W',\n"," 'X',\n"," 'Y',\n"," 'Z',\n"," '[',\n"," ']',\n"," '_',\n"," '`',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z',\n"," '|',\n"," '}'}"]},"metadata":{},"execution_count":8}],"source":["all_characters = set(text)\n","all_characters"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9W15zSVBz0j","executionInfo":{"status":"ok","timestamp":1643618919185,"user_tz":-240,"elapsed":7,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"f6418cae-cab0-4397-e617-4335b752fea6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'i',\n"," 1: 'm',\n"," 2: 'a',\n"," 3: ']',\n"," 4: '\"',\n"," 5: 'J',\n"," 6: 'q',\n"," 7: 'c',\n"," 8: ';',\n"," 9: 'P',\n"," 10: 'W',\n"," 11: 'v',\n"," 12: '>',\n"," 13: 'y',\n"," 14: 'O',\n"," 15: '}',\n"," 16: 'e',\n"," 17: 'V',\n"," 18: '[',\n"," 19: 'E',\n"," 20: 'd',\n"," 21: '2',\n"," 22: '\\n',\n"," 23: 'U',\n"," 24: 'j',\n"," 25: 'o',\n"," 26: 'G',\n"," 27: 'C',\n"," 28: ':',\n"," 29: 'x',\n"," 30: 'L',\n"," 31: '_',\n"," 32: 'u',\n"," 33: '&',\n"," 34: ')',\n"," 35: 'D',\n"," 36: 'A',\n"," 37: 'B',\n"," 38: 'l',\n"," 39: ' ',\n"," 40: '1',\n"," 41: 'I',\n"," 42: '(',\n"," 43: ',',\n"," 44: '3',\n"," 45: 's',\n"," 46: 'w',\n"," 47: 'S',\n"," 48: 'f',\n"," 49: 'Q',\n"," 50: '`',\n"," 51: '?',\n"," 52: '8',\n"," 53: 'K',\n"," 54: '-',\n"," 55: 'F',\n"," 56: '!',\n"," 57: '.',\n"," 58: 'Y',\n"," 59: 'R',\n"," 60: '9',\n"," 61: '0',\n"," 62: 'T',\n"," 63: 'n',\n"," 64: 'h',\n"," 65: 'M',\n"," 66: 'z',\n"," 67: \"'\",\n"," 68: '6',\n"," 69: '7',\n"," 70: 'k',\n"," 71: 'b',\n"," 72: 'H',\n"," 73: 'p',\n"," 74: '<',\n"," 75: '4',\n"," 76: 'Z',\n"," 77: 'X',\n"," 78: '|',\n"," 79: 'r',\n"," 80: '5',\n"," 81: 'N',\n"," 82: 't',\n"," 83: 'g'}"]},"metadata":{},"execution_count":9}],"source":["decoder = dict(enumerate(all_characters))\n","decoder"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3b5hA7U7Bz0m","executionInfo":{"status":"ok","timestamp":1643618921693,"user_tz":-240,"elapsed":3,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"d86ad210-9877-4ff0-e020-4f32553209ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_items([(0, 'i'), (1, 'm'), (2, 'a'), (3, ']'), (4, '\"'), (5, 'J'), (6, 'q'), (7, 'c'), (8, ';'), (9, 'P'), (10, 'W'), (11, 'v'), (12, '>'), (13, 'y'), (14, 'O'), (15, '}'), (16, 'e'), (17, 'V'), (18, '['), (19, 'E'), (20, 'd'), (21, '2'), (22, '\\n'), (23, 'U'), (24, 'j'), (25, 'o'), (26, 'G'), (27, 'C'), (28, ':'), (29, 'x'), (30, 'L'), (31, '_'), (32, 'u'), (33, '&'), (34, ')'), (35, 'D'), (36, 'A'), (37, 'B'), (38, 'l'), (39, ' '), (40, '1'), (41, 'I'), (42, '('), (43, ','), (44, '3'), (45, 's'), (46, 'w'), (47, 'S'), (48, 'f'), (49, 'Q'), (50, '`'), (51, '?'), (52, '8'), (53, 'K'), (54, '-'), (55, 'F'), (56, '!'), (57, '.'), (58, 'Y'), (59, 'R'), (60, '9'), (61, '0'), (62, 'T'), (63, 'n'), (64, 'h'), (65, 'M'), (66, 'z'), (67, \"'\"), (68, '6'), (69, '7'), (70, 'k'), (71, 'b'), (72, 'H'), (73, 'p'), (74, '<'), (75, '4'), (76, 'Z'), (77, 'X'), (78, '|'), (79, 'r'), (80, '5'), (81, 'N'), (82, 't'), (83, 'g')])"]},"metadata":{},"execution_count":10}],"source":["decoder.items()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"lYFztwqeBz0n","executionInfo":{"status":"ok","timestamp":1643618924051,"user_tz":-240,"elapsed":3,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["encoder = {char: ind for ind,char in decoder.items()}"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lc2Ko3FuBz0o","executionInfo":{"status":"ok","timestamp":1643618926918,"user_tz":-240,"elapsed":7,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"7a524bac-320a-452c-b2ec-c48c30ad44a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'\\n': 22,\n"," ' ': 39,\n"," '!': 56,\n"," '\"': 4,\n"," '&': 33,\n"," \"'\": 67,\n"," '(': 42,\n"," ')': 34,\n"," ',': 43,\n"," '-': 54,\n"," '.': 57,\n"," '0': 61,\n"," '1': 40,\n"," '2': 21,\n"," '3': 44,\n"," '4': 75,\n"," '5': 80,\n"," '6': 68,\n"," '7': 69,\n"," '8': 52,\n"," '9': 60,\n"," ':': 28,\n"," ';': 8,\n"," '<': 74,\n"," '>': 12,\n"," '?': 51,\n"," 'A': 36,\n"," 'B': 37,\n"," 'C': 27,\n"," 'D': 35,\n"," 'E': 19,\n"," 'F': 55,\n"," 'G': 26,\n"," 'H': 72,\n"," 'I': 41,\n"," 'J': 5,\n"," 'K': 53,\n"," 'L': 30,\n"," 'M': 65,\n"," 'N': 81,\n"," 'O': 14,\n"," 'P': 9,\n"," 'Q': 49,\n"," 'R': 59,\n"," 'S': 47,\n"," 'T': 62,\n"," 'U': 23,\n"," 'V': 17,\n"," 'W': 10,\n"," 'X': 77,\n"," 'Y': 58,\n"," 'Z': 76,\n"," '[': 18,\n"," ']': 3,\n"," '_': 31,\n"," '`': 50,\n"," 'a': 2,\n"," 'b': 71,\n"," 'c': 7,\n"," 'd': 20,\n"," 'e': 16,\n"," 'f': 48,\n"," 'g': 83,\n"," 'h': 64,\n"," 'i': 0,\n"," 'j': 24,\n"," 'k': 70,\n"," 'l': 38,\n"," 'm': 1,\n"," 'n': 63,\n"," 'o': 25,\n"," 'p': 73,\n"," 'q': 6,\n"," 'r': 79,\n"," 's': 45,\n"," 't': 82,\n"," 'u': 32,\n"," 'v': 11,\n"," 'w': 46,\n"," 'x': 29,\n"," 'y': 13,\n"," 'z': 66,\n"," '|': 78,\n"," '}': 15}"]},"metadata":{},"execution_count":12}],"source":["encoder"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qcAcFwdBBz0o","executionInfo":{"status":"ok","timestamp":1643618930975,"user_tz":-240,"elapsed":955,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"7866ab57-d369-4c36-fef3-4d3776897dc2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([22, 39, 39, ..., 19, 81, 35])"]},"metadata":{},"execution_count":13}],"source":["encoded_text = np.array([encoder[char] for char in text])\n","encoded_text"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAp3hdVkBz0p","executionInfo":{"status":"ok","timestamp":1643618933854,"user_tz":-240,"elapsed":5,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"b91aff9e-54af-41d8-e36c-e4a87a261e52"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([22, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39,\n","       39, 39, 39, 39, 39, 40, 22, 39, 39, 55, 79, 25,  1, 39, 48,  2,  0,\n","       79, 16, 45, 82, 39,  7, 79, 16,  2, 82, 32, 79, 16, 45, 39, 46, 16,\n","       39, 20, 16, 45,  0, 79, 16, 39,  0, 63,  7, 79, 16,  2, 45, 16, 43,\n","       22, 39, 39, 62, 64,  2, 82, 39, 82, 64, 16, 79, 16, 71, 13, 39, 71,\n","       16,  2, 32, 82, 13, 67, 45, 39, 79, 25, 45, 16, 39,  1,  0, 83, 64,\n","       82, 39, 63, 16, 11, 16, 79, 39, 20,  0, 16, 43, 22, 39, 39, 37, 32,\n","       82, 39,  2, 45, 39, 82, 64, 16, 39, 79,  0, 73, 16, 79, 39, 45, 64,\n","       25, 32, 38, 20, 39, 71, 13, 39, 82,  0,  1, 16, 39, 20, 16,  7, 16,\n","        2, 45, 16, 43, 22, 39, 39, 72,  0, 45, 39, 82, 16, 63, 20, 16, 79,\n","       39, 64, 16,  0, 79, 39,  1,  0, 83, 64, 82, 39, 71, 16,  2, 79, 39,\n","       64,  0, 45, 39,  1, 16,  1, 25, 79, 13, 28, 22, 39, 39, 37, 32, 82,\n","       39, 82, 64, 25, 32, 39,  7, 25, 63, 82, 79,  2,  7, 82, 16, 20, 39,\n","       82, 25, 39, 82, 64,  0, 63, 16, 39, 25, 46, 63, 39, 71, 79,  0, 83,\n","       64, 82, 39, 16, 13, 16, 45, 43, 22, 39, 39, 55, 16, 16, 20, 67, 45,\n","       82, 39, 82, 64, 13, 39, 38,  0, 83, 64, 82, 67, 45, 39, 48, 38,  2,\n","        1, 16, 39, 46,  0, 82, 64, 39, 45, 16, 38, 48, 54, 45, 32, 71, 45,\n","       82,  2, 63, 82,  0,  2, 38, 39, 48, 32, 16, 38, 43, 22, 39, 39, 65,\n","        2, 70,  0, 63, 83, 39,  2, 39, 48,  2,  1,  0, 63, 16, 39, 46, 64,\n","       16, 79, 16, 39,  2, 71, 32, 63, 20,  2, 63,  7, 16, 39, 38,  0, 16,\n","       45, 43, 22, 39, 39, 62, 64, 13, 39, 45, 16, 38, 48, 39, 82, 64, 13,\n","       39, 48, 25, 16, 43, 39, 82, 25, 39, 82, 64, 13, 39, 45, 46, 16, 16,\n","       82, 39, 45, 16, 38, 48, 39, 82, 25, 25, 39,  7, 79, 32, 16, 38, 28,\n","       22, 39, 39, 62, 64, 25, 32, 39, 82, 64,  2, 82, 39,  2, 79, 82, 39,\n","       63, 25, 46, 39, 82, 64, 16, 39, 46, 25, 79, 38, 20, 67, 45, 39, 48,\n","       79, 16, 45, 64, 39, 25, 79, 63,  2,  1, 16, 63, 82, 43, 22, 39, 39,\n","       36, 63, 20, 39, 25, 63, 38, 13, 39, 64, 16, 79,  2, 38, 20, 39, 82,\n","       25, 39, 82, 64, 16, 39, 83,  2, 32, 20, 13, 39, 45, 73, 79,  0, 63,\n","       83, 43, 22, 39, 39, 10,  0, 82, 64,  0, 63, 39, 82, 64,  0, 63, 16,\n","       39, 25, 46, 63, 39, 71, 32])"]},"metadata":{},"execution_count":14}],"source":["encoded_text[:500]"]},{"cell_type":"code","source":["decoder[39]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"pRkWcCN2P81m","executionInfo":{"status":"ok","timestamp":1643618946808,"user_tz":-240,"elapsed":12,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"e955e49e-3f09-45b5-e8b4-01e2e3a8e5eb"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' '"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"auboTaQqBz0p"},"source":["## One Hot Encoding\n","\n","As previously discussed, we need to one-hot encode our data inorder for it to work with the network structure. Make sure to review numpy if any of these operations confuse you!"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"gP0OX_fzBz0q","executionInfo":{"status":"ok","timestamp":1643618950617,"user_tz":-240,"elapsed":4,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["def one_hot_encoder(encoded_text, num_uni_chars):\n","    '''\n","    encoded_text : batch of encoded text\n","    \n","    num_uni_chars = number of unique characters (len(set(text)))\n","    '''\n","    \n","    # METHOD FROM:\n","    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n","      \n","    # Create a placeholder for zeros.\n","    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n","    \n","    # Convert data type for later use with pytorch (errors if we dont!)\n","    one_hot = one_hot.astype(np.float32)\n","\n","    # Using fancy indexing fill in the 1s at the correct index locations\n","    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n","    \n","\n","    # Reshape it so it matches the batch sahe\n","    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n","    \n","    return one_hot"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dWkCfy2qBz0r","executionInfo":{"status":"ok","timestamp":1643618952473,"user_tz":-240,"elapsed":7,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"8f6fc104-de1f-41bd-947b-bcc64d65a41d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.]], dtype=float32)"]},"metadata":{},"execution_count":18}],"source":["one_hot_encoder(np.array([1,2,0,1]),3)"]},{"cell_type":"markdown","metadata":{"id":"Pyt86RO1Bz0r"},"source":["--------------\n","---------------\n","# Creating Training Batches\n","\n","We need to create a function that will generate batches of characters along with the next character in the sequence as a label.\n","\n","-----------------\n","------------"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"TswQI0ubBz0t","executionInfo":{"status":"ok","timestamp":1643618962465,"user_tz":-240,"elapsed":4,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["example_text = np.arange(10)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c9p9fQPhBz0v","executionInfo":{"status":"ok","timestamp":1643618966135,"user_tz":-240,"elapsed":31,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"7322e997-1567-460a-d6d8-6e02a0e0758d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"metadata":{},"execution_count":20}],"source":["example_text"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"go3O_cJlBz0x","executionInfo":{"status":"ok","timestamp":1643618968559,"user_tz":-240,"elapsed":8,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"6da97e17-1376-41dd-d5de-bdd57ed3e36a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1],\n","       [2, 3],\n","       [4, 5],\n","       [6, 7],\n","       [8, 9]])"]},"metadata":{},"execution_count":21}],"source":["# If we wanted 5 batches\n","example_text.reshape((5,-1))"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"M75Nf8buBz0y","executionInfo":{"status":"ok","timestamp":1643618972252,"user_tz":-240,"elapsed":29,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n","    \n","    '''\n","    Generate (using yield) batches for training.\n","    \n","    X: Encoded Text of length seq_len\n","    Y: Encoded Text shifted by one\n","    \n","    Example:\n","    \n","    X:\n","    \n","    [[1 2 3]]\n","    \n","    Y:\n","    \n","    [[ 2 3 4]]\n","    \n","    encoded_text : Complete Encoded Text to make batches from\n","    batch_size : Number of samples per batch\n","    seq_len : Length of character sequence\n","       \n","    '''\n","    \n","    # Total number of characters per batch\n","    # Example: If samp_per_batch is 2 and seq_len is 50, then 100 characters come out per batch.\n","    char_per_batch = samp_per_batch * seq_len\n","    \n","    \n","    # Number of batches available to make\n","    # Use int() to roun to nearest integer\n","    num_batches_avail = int(len(encoded_text)/char_per_batch)\n","    \n","    # Cut off end of encoded_text that won't fit evenly into a batch\n","    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n","    \n","    \n","    # Reshape text into rows the size of a batch\n","    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n","    \n","\n","    # Go through each row in array.\n","    for n in range(0, encoded_text.shape[1], seq_len):\n","        \n","        # Grab feature characters\n","        x = encoded_text[:, n:n+seq_len]\n","        \n","        # y is the target shifted over by 1\n","        y = np.zeros_like(x)\n","       \n","        #\n","        try:\n","            y[:, :-1] = x[:, 1:]\n","            y[:, -1]  = encoded_text[:, n+seq_len]\n","            \n","        # FOR POTENTIAL INDEXING ERROR AT THE END    \n","        except:\n","            y[:, :-1] = x[:, 1:]\n","            y[:, -1] = encoded_text[:, 0]\n","            \n","        yield x, y"]},{"cell_type":"markdown","metadata":{"id":"XRkLyl9JBz0z"},"source":["### Example of generating a batch"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"0iwqxupxBz00","executionInfo":{"status":"ok","timestamp":1643618975459,"user_tz":-240,"elapsed":5,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["sample_text = np.arange(20) # encoded_text[:20]"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpPJADY-Bz00","executionInfo":{"status":"ok","timestamp":1643618977631,"user_tz":-240,"elapsed":7,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"32094add-fa7c-41bc-afb5-af54ae6f14c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n","       17, 18, 19])"]},"metadata":{},"execution_count":24}],"source":["sample_text"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"fqtKYrw3Bz01","executionInfo":{"status":"ok","timestamp":1643618981310,"user_tz":-240,"elapsed":5,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"POlEJJVUBz01","executionInfo":{"status":"ok","timestamp":1643618984241,"user_tz":-240,"elapsed":4,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["# Grab first batch\n","x, y = next(batch_generator)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXjGJl1ZBz01","executionInfo":{"status":"ok","timestamp":1643618987255,"user_tz":-240,"elapsed":6,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"8e0c7a99-c8bb-4b85-b7ab-1c240915d8cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  1,  2,  3,  4],\n","       [10, 11, 12, 13, 14]])"]},"metadata":{},"execution_count":27}],"source":["x"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anvLjnBCBz02","executionInfo":{"status":"ok","timestamp":1643618990243,"user_tz":-240,"elapsed":6,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"aa80df8d-8bd3-4159-add6-d2203e5e720c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  2,  3,  4,  5],\n","       [11, 12, 13, 14, 15]])"]},"metadata":{},"execution_count":28}],"source":["y"]},{"cell_type":"code","source":["x, y = next(batch_generator)"],"metadata":{"id":"w8vw8SctTiHf","executionInfo":{"status":"ok","timestamp":1643618993514,"user_tz":-240,"elapsed":4,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OrE9ZPF3Tj2B","executionInfo":{"status":"ok","timestamp":1643618997225,"user_tz":-240,"elapsed":6,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"5cfce86e-2932-4ee0-a5c6-64ffa7176df1"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 5,  6,  7,  8,  9],\n","       [15, 16, 17, 18, 19]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zG2DCnEVTmBu","executionInfo":{"status":"ok","timestamp":1643619002310,"user_tz":-240,"elapsed":1048,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"6ec5ed80-c2c1-4c16-9925-0ab8a7615ace"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 6,  7,  8,  9,  0],\n","       [16, 17, 18, 19, 10]])"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"Cg9JCUxyBz03"},"source":["--------"]},{"cell_type":"markdown","metadata":{"id":"-jswSS3QBz03"},"source":["## GPU Check\n","\n","Remember this will take a lot longer on CPU!"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tB1f4ZpBz03","executionInfo":{"status":"ok","timestamp":1643619009394,"user_tz":-240,"elapsed":1176,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"8169623d-4bed-43f1-98cb-850dd747c9cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":32}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"JQLqPtfgBz04"},"source":["# Creating the LSTM Model\n","\n","**Note! We will have options for GPU users and CPU users. CPU will take MUCH LONGER to train and you may encounter RAM issues depending on your hardware. If that is the case, consider using cloud services like AWS, GCP, or Azure. Note, these may cost you money to use!**"]},{"cell_type":"markdown","source":["https://discuss.pytorch.org/uploads/default/original/2X/d/d8688e43375fd7a246b5d64047b64393d80983be.png"],"metadata":{"id":"6ZZYWBEOfZxI"}},{"cell_type":"code","execution_count":33,"metadata":{"id":"fT6Pr56EBz04","executionInfo":{"status":"ok","timestamp":1643622243213,"user_tz":-240,"elapsed":437,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["# num_layers in RNN is just stacking RNNs on top of each other. \n","# So you get a hidden from each layer and an output only from the topmost layer\n","\n","# The output for the LSTM is the output for all the hidden nodes on the final layer.\n","# hidden_size - the number of LSTM blocks per layer.\n","#            - the number of hidden features in hidden state h\n","# input_size - the number of input features per time-step.\n","#            - the number of expected features in input x\n","# num_layers - the number of hidden layers.\n","\n","# Input of LSTM nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n","                            # num_layers=num_layers, batch_first=True)\n","\n","# In total there are hidden_size * num_layers LSTM blocks.\n","\n","# The input dimensions are (seq_len, batch, input_size).\n","# seq_len - the number of time steps in each input stream.\n","# batch - the size of each batch of input sequences.\n","\n","# The hidden and cell dimensions are: (num_layers, batch, hidden_size)\n","\n","# output shape (seq_len, batch, hidden_size * num_directions): \n","# tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n","\n","# So there will be hidden_size * num_directions outputs. You didn't initialise the RNN to be bidirectional so num_directions is 1. So output_size = hidden_size.\n","\n","\n","class CharModel(nn.Module):\n","    \n","    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n","        \n","        \n","        # SET UP ATTRIBUTES\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.num_layers = num_layers\n","        self.num_hidden = num_hidden\n","        self.use_gpu = use_gpu\n","        \n","        #CHARACTER SET, ENCODER, and DECODER\n","        self.all_chars = all_chars\n","        self.decoder = dict(enumerate(all_chars))\n","        self.encoder = {char: ind for ind,char in decoder.items()}\n","        \n","        # LSTM input (h0,c0) of shape (sequence length, batch and input_size) \n","        # If h0 and c0 not provided they default to 0\n","        # Hidden_State or h0 and cel dimension c0 of shape (num_layers*num_directions, batch, hidden_size)\n","        # Output (h_n,c_n) of shape (seq_len, batch, num_directions * hidden_size)\n","\n","        # If batch=True, input and output provided in this format (batch, seq, feature)\n","        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n","        \n","        self.dropout = nn.Dropout(drop_prob)\n","        \n","        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n","      \n","    \n","    def forward(self, x, hidden):\n","                  \n","        \n","        lstm_output, hidden = self.lstm(x, hidden)\n","        \n","        \n","        drop_output = self.dropout(lstm_output)\n","        \n","        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n","        \n","        \n","        final_out = self.fc_linear(drop_output)\n","        \n","        \n","        return final_out, hidden\n","    \n","    \n","    def hidden_state(self, batch_size):\n","        '''\n","        Used as separate method to account for both GPU and CPU users.\n","        '''\n","        \n","        if self.use_gpu:\n","            # The hidden and cell dimensions are: (num_layers, batch, hidden_size)\n","            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n","                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n","        else:\n","            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n","                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n","        \n","        return hidden\n","        "]},{"cell_type":"markdown","metadata":{"id":"rK8vUdDSBz05"},"source":["## Instance of the Model"]},{"cell_type":"code","source":["len(all_characters)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dan7_QIDhTs3","executionInfo":{"status":"ok","timestamp":1643622263453,"user_tz":-240,"elapsed":8,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"219f668d-8db2-476c-c164-9def242bf07a"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["84"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","execution_count":35,"metadata":{"id":"1MhYXoAuBz05","executionInfo":{"status":"ok","timestamp":1643622285498,"user_tz":-240,"elapsed":417,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["model = CharModel(\n","    all_chars=all_characters,\n","    num_hidden=512,\n","    num_layers=3,\n","    drop_prob=0.5,\n","    use_gpu=True,\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mh61FiVDBz05","executionInfo":{"status":"ok","timestamp":1643622336109,"user_tz":-240,"elapsed":438,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"30aa3122-1860-429a-bb48-45048aedaedb"},"outputs":[{"output_type":"stream","name":"stdout","text":["172032\n","1048576\n","2048\n","2048\n","1048576\n","1048576\n","2048\n","2048\n","1048576\n","1048576\n","2048\n","2048\n","43008\n","84\n"]}],"source":["total_param  = []\n","for p in model.parameters():\n","  print(int(p.numel()))\n","  total_param.append(int(p.numel()))"]},{"cell_type":"markdown","metadata":{"id":"uYzH4WaFBz06"},"source":["Try to make the total_parameters be roughly the same magnitude as the number of characters in the text."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2CV6n2EBz06","executionInfo":{"status":"ok","timestamp":1643622345619,"user_tz":-240,"elapsed":409,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"214e2278-f929-47e8-c8ec-bcd9f4073a1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5470292"]},"metadata":{},"execution_count":37}],"source":["sum(total_param)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GVWCJBm2Bz06","executionInfo":{"status":"ok","timestamp":1643622347691,"user_tz":-240,"elapsed":399,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"94ce88a0-996a-4dde-8f7d-df3a0ac20e38"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5445609"]},"metadata":{},"execution_count":38}],"source":["len(encoded_text)"]},{"cell_type":"code","source":["# Choose hidden_size based such that number of parameters equals number of text"],"metadata":{"id":"54HzEEbfh1lR","executionInfo":{"status":"ok","timestamp":1643622428661,"user_tz":-240,"elapsed":394,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4CBpp6mBz06"},"source":["### Optimizer and Loss"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"II3H_HcCBz07","executionInfo":{"status":"ok","timestamp":1643622430977,"user_tz":-240,"elapsed":411,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"ATg5HTB8Bz07"},"source":["## Training Data and Validation Data"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"70MM1gVXBz07","executionInfo":{"status":"ok","timestamp":1643622436941,"user_tz":-240,"elapsed":411,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["# percentage of data to be used for training\n","train_percent = 0.1"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORbWrsKZBz08","executionInfo":{"status":"ok","timestamp":1643622438268,"user_tz":-240,"elapsed":7,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"85d51a79-b56a-4e3c-a253-4474bdabd679"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5445609"]},"metadata":{},"execution_count":42}],"source":["len(encoded_text)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3J0e5reBz08","executionInfo":{"status":"ok","timestamp":1643622439761,"user_tz":-240,"elapsed":6,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"23ab63d0-f6fb-41f9-d849-38e6386f7f22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["544560"]},"metadata":{},"execution_count":43}],"source":["int(len(encoded_text) * (train_percent))"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"wJmIBaKZBz09","executionInfo":{"status":"ok","timestamp":1643622444406,"user_tz":-240,"elapsed":402,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["train_ind = int(len(encoded_text) * (train_percent))"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"Wt2r9xczBz0_","executionInfo":{"status":"ok","timestamp":1643622447010,"user_tz":-240,"elapsed":713,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["train_data = encoded_text[:train_ind]\n","val_data = encoded_text[train_ind:]"]},{"cell_type":"code","source":["train_data.shape, val_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODTwNmZwiEnb","executionInfo":{"status":"ok","timestamp":1643622465532,"user_tz":-240,"elapsed":405,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}},"outputId":"5e3b2710-f3da-4d50-8882-fb2b2ba28d22"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((544560,), (4901049,))"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"NoQua0OUBz1B"},"source":["# Training the Network"]},{"cell_type":"markdown","metadata":{"id":"r_ugV2fDBz1C"},"source":["## Variables\n","\n","Feel free to play around with these values!"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"FZL7stV8Bz1D","executionInfo":{"status":"ok","timestamp":1643622613260,"user_tz":-240,"elapsed":1380,"user":{"displayName":"Umar Salman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHZcOsjv11SLF2pszrnnYSQho-DRZsHsPpvm1oXQ=s64","userId":"02559536710924243729"}}},"outputs":[],"source":["## VARIABLES\n","\n","# Epochs to train for\n","epochs = 50\n","# batch size \n","batch_size = 128\n","\n","# Length of sequence\n","seq_len = 100\n","\n","# for printing report purposes\n","# always start at 0\n","tracker = 0\n","\n","# number of characters in text\n","num_char = max(encoded_text)+1"]},{"cell_type":"markdown","metadata":{"id":"sdAjbZ9xBz1E"},"source":["------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16KUYjfgBz1E","outputId":"ab741cb6-80a8-464d-ad78-1fd5b9d43fc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 Step: 25 Val Loss: 3.241183280944824\n","Epoch: 1 Step: 50 Val Loss: 3.2209343910217285\n","Epoch: 1 Step: 75 Val Loss: 3.2246036529541016\n","Epoch: 2 Step: 100 Val Loss: 3.103549003601074\n","Epoch: 2 Step: 125 Val Loss: 3.0078160762786865\n","Epoch: 3 Step: 150 Val Loss: 2.8424694538116455\n","Epoch: 4 Step: 175 Val Loss: 2.7311224937438965\n","Epoch: 4 Step: 200 Val Loss: 2.6245357990264893\n","Epoch: 5 Step: 225 Val Loss: 2.530056953430176\n","Epoch: 5 Step: 250 Val Loss: 2.511744737625122\n","Epoch: 6 Step: 275 Val Loss: 2.424506187438965\n","Epoch: 7 Step: 300 Val Loss: 2.3757734298706055\n","Epoch: 7 Step: 325 Val Loss: 2.3281121253967285\n","Epoch: 8 Step: 350 Val Loss: 2.287860631942749\n","Epoch: 8 Step: 375 Val Loss: 2.258666515350342\n","Epoch: 9 Step: 400 Val Loss: 2.219432830810547\n","Epoch: 10 Step: 425 Val Loss: 2.1962826251983643\n","Epoch: 10 Step: 450 Val Loss: 2.1531155109405518\n","Epoch: 11 Step: 475 Val Loss: 2.12485408782959\n","Epoch: 11 Step: 500 Val Loss: 2.102055072784424\n","Epoch: 12 Step: 525 Val Loss: 2.0815775394439697\n","Epoch: 13 Step: 550 Val Loss: 2.065098524093628\n","Epoch: 13 Step: 575 Val Loss: 2.045565366744995\n","Epoch: 14 Step: 600 Val Loss: 2.024740695953369\n","Epoch: 14 Step: 625 Val Loss: 2.002650737762451\n","Epoch: 15 Step: 650 Val Loss: 1.9918841123580933\n","Epoch: 16 Step: 675 Val Loss: 1.973698616027832\n","Epoch: 16 Step: 700 Val Loss: 1.9563044309616089\n","Epoch: 17 Step: 725 Val Loss: 1.941154956817627\n","Epoch: 17 Step: 750 Val Loss: 1.9296284914016724\n","Epoch: 18 Step: 775 Val Loss: 1.9155606031417847\n","Epoch: 19 Step: 800 Val Loss: 1.9066412448883057\n","Epoch: 19 Step: 825 Val Loss: 1.8944147825241089\n","Epoch: 20 Step: 850 Val Loss: 1.8825374841690063\n","Epoch: 20 Step: 875 Val Loss: 1.8753138780593872\n","Epoch: 21 Step: 900 Val Loss: 1.8679431676864624\n","Epoch: 22 Step: 925 Val Loss: 1.8626611232757568\n","Epoch: 22 Step: 950 Val Loss: 1.8534228801727295\n","Epoch: 23 Step: 975 Val Loss: 1.8416558504104614\n","Epoch: 23 Step: 1000 Val Loss: 1.8408966064453125\n","Epoch: 24 Step: 1025 Val Loss: 1.832461953163147\n","Epoch: 24 Step: 1050 Val Loss: 1.8274987936019897\n","Epoch: 25 Step: 1075 Val Loss: 1.8215422630310059\n","Epoch: 26 Step: 1100 Val Loss: 1.8141027688980103\n","Epoch: 26 Step: 1125 Val Loss: 1.8090591430664062\n","Epoch: 27 Step: 1150 Val Loss: 1.808109998703003\n","Epoch: 27 Step: 1175 Val Loss: 1.798502802848816\n","Epoch: 28 Step: 1200 Val Loss: 1.8020660877227783\n","Epoch: 29 Step: 1225 Val Loss: 1.7935495376586914\n","Epoch: 29 Step: 1250 Val Loss: 1.7842048406600952\n","Epoch: 30 Step: 1275 Val Loss: 1.7775088548660278\n","Epoch: 30 Step: 1300 Val Loss: 1.7796084880828857\n","Epoch: 31 Step: 1325 Val Loss: 1.778605341911316\n","Epoch: 32 Step: 1350 Val Loss: 1.778555154800415\n","Epoch: 32 Step: 1375 Val Loss: 1.7726141214370728\n","Epoch: 33 Step: 1400 Val Loss: 1.7713408470153809\n","Epoch: 33 Step: 1425 Val Loss: 1.7647587060928345\n","Epoch: 34 Step: 1450 Val Loss: 1.7639307975769043\n","Epoch: 35 Step: 1475 Val Loss: 1.7668451070785522\n","Epoch: 35 Step: 1500 Val Loss: 1.7553269863128662\n","Epoch: 36 Step: 1525 Val Loss: 1.7537274360656738\n","Epoch: 36 Step: 1550 Val Loss: 1.7476931810379028\n","Epoch: 37 Step: 1575 Val Loss: 1.7471405267715454\n","Epoch: 38 Step: 1600 Val Loss: 1.748685359954834\n","Epoch: 38 Step: 1625 Val Loss: 1.7501276731491089\n","Epoch: 39 Step: 1650 Val Loss: 1.7491378784179688\n","Epoch: 39 Step: 1675 Val Loss: 1.73957097530365\n","Epoch: 40 Step: 1700 Val Loss: 1.7412303686141968\n","Epoch: 41 Step: 1725 Val Loss: 1.7421422004699707\n","Epoch: 41 Step: 1750 Val Loss: 1.7420353889465332\n","Epoch: 42 Step: 1775 Val Loss: 1.732686161994934\n","Epoch: 42 Step: 1800 Val Loss: 1.7336872816085815\n","Epoch: 43 Step: 1825 Val Loss: 1.7360546588897705\n","Epoch: 44 Step: 1850 Val Loss: 1.7357029914855957\n","Epoch: 44 Step: 1875 Val Loss: 1.736457109451294\n","Epoch: 45 Step: 1900 Val Loss: 1.7330776453018188\n","Epoch: 45 Step: 1925 Val Loss: 1.7337615489959717\n","Epoch: 46 Step: 1950 Val Loss: 1.738358736038208\n","Epoch: 47 Step: 1975 Val Loss: 1.7346129417419434\n","Epoch: 47 Step: 2000 Val Loss: 1.743545413017273\n","Epoch: 48 Step: 2025 Val Loss: 1.7326579093933105\n","Epoch: 48 Step: 2050 Val Loss: 1.7226899862289429\n","Epoch: 49 Step: 2075 Val Loss: 1.7329885959625244\n","Epoch: 49 Step: 2100 Val Loss: 1.7302632331848145\n"]}],"source":["# Set model to train\n","model.train()\n","\n","\n","# Check to see if using GPU\n","if model.use_gpu:\n","    model.cuda()\n","\n","for i in range(epochs):\n","    \n","    hidden = model.hidden_state(batch_size)\n","    \n","    \n","    for x,y in generate_batches(train_data,batch_size,seq_len):\n","        \n","        tracker += 1\n","        \n","        # One Hot Encode incoming data\n","        x = one_hot_encoder(x,num_char)\n","        \n","        # Convert Numpy Arrays to Tensor\n","        \n","        inputs = torch.from_numpy(x)\n","        targets = torch.from_numpy(y)\n","        \n","        # Adjust for GPU if necessary\n","        \n","        if model.use_gpu:\n","            \n","            inputs = inputs.cuda()\n","            targets = targets.cuda()\n","            \n","        # Reset Hidden State\n","        # If we dont' reset we would backpropagate through all training history\n","        hidden = tuple([state.data for state in hidden])\n","        \n","        model.zero_grad()\n","        \n","        lstm_output, hidden = model.forward(inputs,hidden)\n","        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n","        \n","        loss.backward()\n","        \n","        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n","        # LET\"S CLIP JUST IN CASE\n","        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n","        \n","        optimizer.step()\n","        \n","        \n","        \n","        ###################################\n","        ### CHECK ON VALIDATION SET ######\n","        #################################\n","        \n","        if tracker % 25 == 0:\n","            \n","            val_hidden = model.hidden_state(batch_size)\n","            val_losses = []\n","            model.eval()\n","            \n","            for x,y in generate_batches(val_data,batch_size,seq_len):\n","                \n","                # One Hot Encode incoming data\n","                x = one_hot_encoder(x,num_char)\n","                \n","\n","                # Convert Numpy Arrays to Tensor\n","\n","                inputs = torch.from_numpy(x)\n","                targets = torch.from_numpy(y)\n","\n","                # Adjust for GPU if necessary\n","\n","                if model.use_gpu:\n","\n","                    inputs = inputs.cuda()\n","                    targets = targets.cuda()\n","                    \n","                # Reset Hidden State\n","                # If we dont' reset we would backpropagate through \n","                # all training history\n","                val_hidden = tuple([state.data for state in val_hidden])\n","                \n","                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n","                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n","        \n","                val_losses.append(val_loss.item())\n","            \n","            # Reset to training model after val for loop\n","            model.train()\n","            \n","            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"]},{"cell_type":"markdown","metadata":{"id":"XAXr4hBUBz1F"},"source":["-------\n","------\n","\n","## Saving the Model\n","\n","https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ry56vcCfBz1G"},"outputs":[],"source":["# Be careful to overwrite our original name file!\n","model_name = 'example.net'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RyYFOe_JBz1H"},"outputs":[],"source":["torch.save(model.state_dict(),model_name)"]},{"cell_type":"markdown","metadata":{"id":"Pz8aF-p_Bz1H"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_u-d5vpUBz1H"},"outputs":[],"source":["# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n","\n","model = CharModel(\n","    all_chars=all_characters,\n","    num_hidden=512,\n","    num_layers=3,\n","    drop_prob=0.5,\n","    use_gpu=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ddZzfBUBz1I","outputId":"6eacdb4a-93c2-4713-8a5f-39a405619c18"},"outputs":[{"data":{"text/plain":["CharModel(\n","  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5)\n","  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",")"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(model_name))\n","model.eval()"]},{"cell_type":"markdown","metadata":{"id":"j3ALWrxlBz1I"},"source":["# Generating Predictions"]},{"cell_type":"markdown","metadata":{"id":"eAwggHP9Bz1I"},"source":["--------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q29JkmLoBz1J"},"outputs":[],"source":["def predict_next_char(model, char, hidden=None, k=1):\n","        \n","        # Encode raw letters with model\n","        encoded_text = model.encoder[char]\n","        \n","        # set as numpy array for one hot encoding\n","        # NOTE THE [[ ]] dimensions!!\n","        encoded_text = np.array([[encoded_text]])\n","        \n","        # One hot encoding\n","        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n","        \n","        # Convert to Tensor\n","        inputs = torch.from_numpy(encoded_text)\n","        \n","        # Check for CPU\n","        if(model.use_gpu):\n","            inputs = inputs.cuda()\n","        \n","        \n","        # Grab hidden states\n","        hidden = tuple([state.data for state in hidden])\n","        \n","        \n","        # Run model and get predicted output\n","        lstm_out, hidden = model(inputs, hidden)\n","\n","        \n","        # Convert lstm_out to probabilities\n","        probs = F.softmax(lstm_out, dim=1).data\n","        \n","        \n","        \n","        if(model.use_gpu):\n","            # move back to CPU to use with numpy\n","            probs = probs.cpu()\n","        \n","        \n","        # k determines how many characters to consider\n","        # for our probability choice.\n","        # https://pytorch.org/docs/stable/torch.html#torch.topk\n","        \n","        # Return k largest probabilities in tensor\n","        probs, index_positions = probs.topk(k)\n","        \n","        \n","        index_positions = index_positions.numpy().squeeze()\n","        \n","        # Create array of probabilities\n","        probs = probs.numpy().flatten()\n","        \n","        # Convert to probabilities per index\n","        probs = probs/probs.sum()\n","        \n","        # randomly choose a character based on probabilities\n","        char = np.random.choice(index_positions, p=probs)\n","       \n","        # return the encoded value of the predicted char and the hidden state\n","        return model.decoder[char], hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZQfcKl3Bz1J"},"outputs":[],"source":["def generate_text(model, size, seed='The', k=1):\n","        \n","      \n","    \n","    # CHECK FOR GPU\n","    if(model.use_gpu):\n","        model.cuda()\n","    else:\n","        model.cpu()\n","    \n","    # Evaluation mode\n","    model.eval()\n","    \n","    # begin output from initial seed\n","    output_chars = [c for c in seed]\n","    \n","    # intiate hidden state\n","    hidden = model.hidden_state(1)\n","    \n","    # predict the next character for every character in seed\n","    for char in seed:\n","        char, hidden = predict_next_char(model, char, hidden, k=k)\n","    \n","    # add initial characters to output\n","    output_chars.append(char)\n","    \n","    # Now generate for size requested\n","    for i in range(size):\n","        \n","        # predict based off very last letter in output_chars\n","        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n","        \n","        # add predicted character\n","        output_chars.append(char)\n","    \n","    # return string of predicted text\n","    return ''.join(output_chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rva7R-aGBz1K","outputId":"82aea43e-dd8e-4001-bb30-497fb04a5f62"},"outputs":[{"name":"stdout","output_type":"stream","text":["The will true and breathed to me.\n","    If thou wert better to the stare and send thee,\n","    Which hath any trives and sound and stretged,\n","    That have the better send of the constance,\n","    That then that thou shaltst but that have seem surpet\n","    And we had been the self-fight and had their strange,\n","    With his sward shall strave a servant state.\n","    Where this't she is that to the wind of held\n","    That have this serve that she he with the child\n","    Which they were beauty of their command strowes\n","    And truth and strength to the serves and song.\n","    If thou say'st he that hath seen this should still\n","    To she with his both shall see him.\n","    The world was a solder thou to heaven with me,\n","    And should this can stay that I heave make\n","    Which his charge in her shames, and to his state.\n","    That have tho stol'd of this starts to have,  \n","    And we and to the cheeks that to the stol'd\n","    To serve the courtier time of that sense is.\n","    In the summer that that shall not,\n","    That he will s\n"]}],"source":["print(generate_text(model, 1000, seed='The ', k=3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5OIwx6PBz1K"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"1. RNN-for-Text-Generation .ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}